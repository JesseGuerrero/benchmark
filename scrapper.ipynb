{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ac15cf8a77e71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T02:39:41.951061Z",
     "start_time": "2025-02-03T02:39:12.153641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in...\n",
      "\n",
      "\n",
      "Login Successful, API Key Received!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Assisted with ChatGPT & the official examples from the M2M API documentation.\n",
    "'''\n",
    "\n",
    "import shutil\n",
    "\n",
    "'''Setup entire script and define raw raster download'''\n",
    "from utils.util import *\n",
    "from utils.voice import notifySelf\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import socket\n",
    "import traceback\n",
    "import argparse\n",
    "\n",
    "# # Create an argument parser\n",
    "# parser = argparse.ArgumentParser(description=\"Process data for a range of years.\")\n",
    "#\n",
    "# # Add arguments for startYear and endYear\n",
    "# parser.add_argument(\n",
    "#     \"--startYear\",\n",
    "#     type=int,\n",
    "#     required=True,\n",
    "#     help=\"The starting year of the data range.\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--endYear\",\n",
    "#     type=int,\n",
    "#     required=True,\n",
    "#     help=\"The ending year of the data range.\"\n",
    "# )\n",
    "# args = parser.parse_args()\n",
    "# startYear, endYear = args.startYear, args.endYear\n",
    "\n",
    "def gatherRawRasters(dataset, year, city, aoi_geodf):\n",
    "    print(f'Gathering {dataset} for {year} in {city}.')\n",
    "    if dataset == 'srtm_v3' and year != 2014:\n",
    "        return\n",
    "    bandNames = {'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'ST_B10', 'ST_EMIS', 'QA_PIXEL'}\n",
    "    for month in range(1, 13):\n",
    "        if month == 1:\n",
    "            notifySelf(f'Starting on year {year} in dataset {dataset} as city {city}...')\n",
    "        #Search for scenes\n",
    "        print(\"Starting month\", month)\n",
    "        clear_folder(unprocessed_dir)\n",
    "        search_payload = createSceneSearchPayload(dataset, aoi_geodf, year, month)\n",
    "        scenes = sendRequest(serviceUrl + \"scene-search\", search_payload, apiKey)\n",
    "        pd.json_normalize(scenes['results'])\n",
    "        if len(scenes['results']) == 0:\n",
    "            print(\"Month for scenes empty, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Collect File IDs\n",
    "        entityIds = [result['entityId'] for result in scenes['results'] if result['options']['bulk']]\n",
    "\n",
    "        # Add to basket\n",
    "        listId = f\"{dataset}_{year}_{str(month)}_{socket.gethostname()}\"\n",
    "        scn_list_add_payload = {\n",
    "            \"listId\": listId,\n",
    "            'idField': 'entityId',\n",
    "            \"entityIds\": entityIds,\n",
    "            \"datasetName\": dataset\n",
    "        }\n",
    "        sendRequest(serviceUrl + \"scene-list-add\", scn_list_add_payload, apiKey)\n",
    "\n",
    "        # Select URL download\n",
    "        download_opt_payload = {\n",
    "            \"listId\": listId,\n",
    "            \"datasetName\": dataset,\n",
    "        }\n",
    "        products = sendRequest(serviceUrl + \"download-options\", download_opt_payload, apiKey)\n",
    "        pd.json_normalize(products)\n",
    "\n",
    "        # Collect File URLs based on product\n",
    "        downloads = []\n",
    "        if 'landsat_ot_c2_l2' in dataset:\n",
    "            for product in products:\n",
    "                if product[\"secondaryDownloads\"]:\n",
    "                    for secDownload in product[\"secondaryDownloads\"]:\n",
    "                        if secDownload[\"bulkAvailable\"] and any(band in secDownload['displayId'] for band in bandNames):\n",
    "                            downloads.append({\"entityId\": secDownload[\"entityId\"], \"productId\": secDownload[\"id\"]})\n",
    "                        if secDownload['displayId'].endswith('_MTL.txt'):\n",
    "                            downloads.append({\"entityId\": secDownload[\"entityId\"], \"productId\": secDownload[\"id\"]})\n",
    "        elif 'srtm_v3' in dataset:\n",
    "            for product in products:\n",
    "                if product[\"bulkAvailable\"] and product[\"entityId\"] and product[\"id\"]:\n",
    "                    downloads.append({\"entityId\": product[\"entityId\"], \"productId\": product[\"id\"]})\n",
    "        elif 'nlcd_collection_lndcov' in dataset:\n",
    "            for product in products:\n",
    "                if product[\"bulkAvailable\"] and product[\"entityId\"] and product[\"id\"]:\n",
    "                    downloads.append({\"entityId\": product[\"entityId\"], \"productId\": product[\"id\"]})\n",
    "        download_req_payload = {\n",
    "            \"downloads\": downloads,\n",
    "            \"label\": listId\n",
    "        }\n",
    "        download_request_results = sendRequest(serviceUrl + \"download-request\", download_req_payload, apiKey)\n",
    "\n",
    "        # Download Files Via URL\n",
    "        if dataset == 'landsat_ot_c2_l2':\n",
    "            results = download_request_results['availableDownloads']\n",
    "            for result in results:\n",
    "                runDownload(threads, result['url'])\n",
    "        elif dataset == 'srtm_v3':\n",
    "            results = download_request_results['preparingDownloads']\n",
    "            for result in results:\n",
    "                runDownload(threads, result['url'])\n",
    "        else:\n",
    "            results = download_request_results['preparingDownloads']\n",
    "            for result in results:\n",
    "                runDownload(threads, result['url'])\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        for file in os.listdir(unprocessed_dir):\n",
    "            if file.endswith('.tar'):\n",
    "                try:\n",
    "                    extract_specific_files(unprocessed_dir + '/' + file, unprocessed_dir)\n",
    "                except:\n",
    "                    print(f'Error: Could not extract file {file}')\n",
    "\n",
    "        # Clear Basket\n",
    "        remove_scnlst_payload = {\"listId\": listId}\n",
    "        sendRequest(serviceUrl + \"scene-list-remove\", remove_scnlst_payload, apiKey)\n",
    "\n",
    "        # Re-project Raster Files\n",
    "        for tif in os.listdir(unprocessed_dir):\n",
    "            if tif.endswith('.tif') or tif.endswith('.TIF'):\n",
    "                temp_path = \".temp\"  # Temporary file path\n",
    "                input_path = unprocessed_dir + '/' + tif\n",
    "                with rasterio.open(input_path) as src:\n",
    "                    try:\n",
    "                        color_map = src.colormap(1)\n",
    "                    except ValueError:\n",
    "                        color_map = None\n",
    "                    with rioxarray.open_rasterio(input_path) as raster:\n",
    "                        raster = raster.rio.reproject(\"EPSG:4326\")\n",
    "                        raster.rio.to_raster(temp_path, driver=\"GTiff\")\n",
    "                if color_map:\n",
    "                    with rasterio.open(temp_path, \"r+\") as dst:\n",
    "                        dst.write_colormap(1, color_map)\n",
    "                os.replace(temp_path, input_path)\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "\n",
    "        # Move Unprocessed Files to Raw Folder\n",
    "        for file in os.listdir(unprocessed_dir):\n",
    "            if \".txt\" in file or \".tif\" in file or \".TIF\" in file:\n",
    "                if '1arc_v3' in file:\n",
    "                    moveToRaw(file, 'DEM', f'{year}-01-01', city)\n",
    "                    continue\n",
    "                if 'Annual_NLCD' in file:\n",
    "                    moveToRaw(file, 'Land_Cover', f'{year}-01-01', city)\n",
    "                    continue\n",
    "                date, band, coordinate = getMetaFromLandsatTIRs(file)\n",
    "                if band in ['MTL', 'B10', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'EMIS', 'PIXEL']:\n",
    "                    moveToRaw(file, 'oli', date, city)\n",
    "        print('Finished moving')\n",
    "\n",
    "        #You only need 1 month for these as they are annual\n",
    "        if dataset == 'nlcd_collection_lndcov' or dataset == 'srtm_v3':\n",
    "            break\n",
    "\n",
    "    #Save progress\n",
    "    with open('./Logs/raw_progress.txt', \"a\") as file:\n",
    "        file.write(str(city) + \":\" + str(year) + \":\" + dataset + \"\\n\")\n",
    "    print('progress written for', city, year, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222a6eb46b49dc8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T04:46:04.543960Z",
     "start_time": "2025-01-22T04:46:03.497665Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.shp\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     cities\u001b[38;5;241m.\u001b[39mappend(file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolygon_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.shp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m     aoi_geodf \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshapefile_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     aoi_geodf \u001b[38;5;241m=\u001b[39m aoi_geodf\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:4326\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m aoi_geodf\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/geopandas/io/file.py:294\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_file_like(filename):\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/geopandas/io/file.py:547\u001b[0m, in \u001b[0;36m_read_file_pyogrio\u001b[0;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keywords are deprecated, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release. You can use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    544\u001b[0m     )\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/pyogrio/geopandas.py:333\u001b[0m, in \u001b[0;36mread_dataframe\u001b[0;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m    331\u001b[0m geometry \u001b[38;5;241m=\u001b[39m shapely\u001b[38;5;241m.\u001b[39mfrom_wkb(geometry, on_invalid\u001b[38;5;241m=\u001b[39mon_invalid)\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGeoDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/geopandas/geodataframe.py:209\u001b[0m, in \u001b[0;36mGeoDataFrame.__init__\u001b[0;34m(self, data, geometry, crs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(geometry, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m geometry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;66;03m# __init__ always creates geometry col named \"geometry\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;66;03m# rename as `set_geometry` respects the given series name\u001b[39;00m\n\u001b[1;32m    207\u001b[0m         geometry \u001b[38;5;241m=\u001b[39m geometry\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_geometry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m geometry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m crs:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssigning CRS to a GeoDataFrame without a geometry column is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported. Supply geometry using the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword argument, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor by providing a DataFrame with column name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeometry\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/geopandas/geodataframe.py:413\u001b[0m, in \u001b[0;36mGeoDataFrame.set_geometry\u001b[0;34m(self, col, drop, inplace, crs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;66;03m# update _geometry_column_name prior to assignment\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;66;03m# to avoid default is None warning\u001b[39;00m\n\u001b[1;32m    412\u001b[0m frame\u001b[38;5;241m.\u001b[39m_geometry_column_name \u001b[38;5;241m=\u001b[39m geo_column_name\n\u001b[0;32m--> 413\u001b[0m \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgeo_column_name\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m level\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m frame\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/geopandas/geodataframe.py:1819\u001b[0m, in \u001b[0;36mGeoDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1815\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1816\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeometry column does not contain geometry.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1817\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1818\u001b[0m         )\n\u001b[0;32m-> 1819\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/pandas/core/frame.py:5260\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5258\u001b[0m \u001b[38;5;66;03m# Using a DataFrame would mean coercing values to one dtype\u001b[39;00m\n\u001b[1;32m   5259\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame)\n\u001b[0;32m-> 5260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   5261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Series):\n\u001b[1;32m   5262\u001b[0m         value \u001b[38;5;241m=\u001b[39m Series(value)\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/pandas/core/dtypes/inference.py:300\u001b[0m, in \u001b[0;36mis_dict_like\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mCheck if the object is dict-like.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03mTrue\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m dict_like_attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__contains__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdict_like_attrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# [GH 25196] exclude classes\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m    303\u001b[0m )\n",
      "File \u001b[0;32m/work/ubh496/.conda/envs/ml2/lib/python3.9/site-packages/pandas/core/dtypes/inference.py:300\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mCheck if the object is dict-like.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03mTrue\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m dict_like_attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeys\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__contains__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28;43mhasattr\u001b[39;49m(obj, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m dict_like_attrs)\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# [GH 25196] exclude classes\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m    303\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = ['landsat_ot_c2_l2', 'srtm_v3', 'nlcd_collection_lndcov']\n",
    "years = [year for year in range(startYear, endYear+1)]\n",
    "\n",
    "# Load city footprints from Esri Living Atlas\n",
    "shapefile_folder = \"./Data/area_shp/\"\n",
    "cities = []\n",
    "aoi_geodfs = []\n",
    "for file in os.listdir(shapefile_folder):\n",
    "    if file.endswith(\".shp\"):\n",
    "        cities.append(file.replace('Polygon_', '').replace('.shp', ''))\n",
    "        aoi_geodf = gpd.read_file(shapefile_folder + file)\n",
    "        aoi_geodf = aoi_geodf.to_crs(\"EPSG:4326\")\n",
    "        if aoi_geodf.empty:\n",
    "            sys.exit(\"Error: Shapefile contains no data.\")\n",
    "        aoi_geodfs.append(aoi_geodf)\n",
    "print(\"Shapefiles loaded successfully.\")\n",
    "\n",
    "for j, dataset in enumerate(datasets):\n",
    "    for year in years:\n",
    "        i = 0\n",
    "        while i < int(len(cities)):\n",
    "            try:\n",
    "                clear_folder(unprocessed_dir)\n",
    "                assert len(os.listdir(unprocessed_dir)) == 0, \"Unprocessed directory is not empty.\"\n",
    "                city, aoi_geodf = cities[i], aoi_geodfs[i]\n",
    "                if os.path.exists('./Logs/raw_progress.txt'):\n",
    "                    with open('./Logs/raw_progress.txt', 'r') as file:\n",
    "                        progress = [line.split(':') for line in file.read().strip().split('\\n')]\n",
    "                    if any(city == instance[0] and str(year) == instance[1] and dataset == instance[2] for instance in progress):\n",
    "                        print(f\"{city}, {year}, {dataset} was gathered in the past.\")\n",
    "                    else:\n",
    "                        gatherRawRasters(dataset, year, city, aoi_geodf)\n",
    "                i +=1\n",
    "            except Exception as e:\n",
    "                print(\"An exception occurred:\")\n",
    "                print(f\"Exception: {e}\")\n",
    "                notifySelf(\"An exception occurred:\")\n",
    "                notifySelf(f\"Exception: {e}\")\n",
    "                traceback.print_exc()  # Print the full stack trace\n",
    "                time.sleep(15)\n",
    "print(\"Gathered raw rasters successfully.\")\n",
    "notifySelf(\"Gathered raw rasters successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e6de4e27945b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T00:22:57.416478Z",
     "start_time": "2025-01-30T00:22:08.387484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering only 1st DEM,LC in scenes...: 100%|██████████| 61983/61983 [00:08<00:00, 7408.45it/s]  \n",
      "Merging DEM and Land_Cover:   0%|          | 0/107 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Los_Angeles_CA/2014-01/Merged_n34_w119_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:   1%|          | 1/107 [00:01<03:28,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Lexington-Fayette_KY/2014-01/Merged_n37_w085_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:   5%|▍         | 5/107 [00:02<00:39,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Columbus_GA/2014-01/Merged_n32_w085_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:   7%|▋         | 7/107 [00:02<00:29,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Detroit_MI/2014-01/Merged_n42_w084_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  10%|█         | 11/107 [00:03<00:18,  5.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Phoenix_AZ/2014-01/Merged_n33_w113_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  11%|█         | 12/107 [00:03<00:23,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Denver_CO/2014-01/Merged_n39_w106_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  17%|█▋        | 18/107 [00:04<00:14,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Cape_Coral_FL/2014-01/Merged_n26_w083_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  19%|█▊        | 20/107 [00:04<00:15,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/New_Orleans_LA/2014-01/Merged_n29_w090_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  20%|█▉        | 21/107 [00:05<00:23,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Henderson_NV/2014-01/Merged_n35_w116_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  21%|██        | 22/107 [00:06<00:33,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Milwaukee_WI/2014-01/Merged_n43_w089_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  21%|██▏       | 23/107 [00:07<00:41,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Scottsdale_AZ/2014-01/Merged_n33_w113_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  25%|██▌       | 27/107 [00:09<00:37,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Charleston_SC/2014-01/Merged_n32_w080_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  26%|██▌       | 28/107 [00:09<00:35,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Louisville_Jefferson_County_metro_government__balance__KY/2014-01/Merged_n37_w086_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  29%|██▉       | 31/107 [00:10<00:23,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Athens-Clarke_County_unified_government__balance__GA/2014-01/Merged_n33_w084_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  31%|███       | 33/107 [00:10<00:21,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/San_Jose_CA/2014-01/Merged_n37_w122_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  32%|███▏      | 34/107 [00:11<00:22,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Columbus_OH/2014-01/Merged_n39_w084_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  33%|███▎      | 35/107 [00:12<00:33,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Durham_NC/2014-01/Merged_n36_w079_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  41%|████      | 44/107 [00:12<00:10,  6.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Bakersfield_CA/2014-01/Merged_n35_w119_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  42%|████▏     | 45/107 [00:12<00:10,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Salt_Lake_City_UT/2014-01/Merged_n40_w112_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  45%|████▍     | 48/107 [00:13<00:09,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Chattanooga_TN/2014-01/Merged_n34_w086_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  47%|████▋     | 50/107 [00:13<00:08,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Kansas_City_MO/2014-01/Merged_n39_w095_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  50%|████▉     | 53/107 [00:15<00:17,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Madison_WI/2014-01/Merged_n42_w090_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  52%|█████▏    | 56/107 [00:15<00:12,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Indianapolis_city__balance__IN/2014-01/Merged_n39_w087_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  55%|█████▌    | 59/107 [00:16<00:10,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Tucson_AZ/2014-01/Merged_n32_w111_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  57%|█████▋    | 61/107 [00:16<00:09,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Memphis_TN/2014-01/Merged_n34_w090_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  58%|█████▊    | 62/107 [00:17<00:13,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Augusta-Richmond_County_consolidated_government__balance__GA/2014-01/Merged_n33_w083_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  60%|█████▉    | 64/107 [00:17<00:13,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Omaha_NE/2014-01/Merged_n41_w096_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  61%|██████    | 65/107 [00:18<00:13,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Charlotte_NC/2014-01/Merged_n35_w081_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  62%|██████▏   | 66/107 [00:18<00:14,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Tulsa_OK/2014-01/Merged_n36_w096_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  67%|██████▋   | 72/107 [00:19<00:06,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/San_Diego_CA/2014-01/Merged_n32_w118_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  68%|██████▊   | 73/107 [00:19<00:07,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/California_City_CA/2014-01/Merged_n35_w119_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  74%|███████▍  | 79/107 [00:21<00:07,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Greensboro_NC/2014-01/Merged_n35_w080_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  77%|███████▋  | 82/107 [00:21<00:05,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/New_York_NY/2014-01/Merged_n40_w075_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  79%|███████▊  | 84/107 [00:22<00:04,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Columbia_SC/2014-01/Merged_n34_w082_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  79%|███████▉  | 85/107 [00:23<00:07,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Philadelphia_PA/2014-01/Merged_n40_w076_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  83%|████████▎ | 89/107 [00:23<00:04,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/North_Port_FL/2014-01/Merged_n26_w083_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  88%|████████▊ | 94/107 [00:24<00:02,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Virginia_Beach_VA/2014-01/Merged_n36_w077_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  89%|████████▉ | 95/107 [00:24<00:02,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Savannah_GA/2014-01/Merged_n32_w082_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  92%|█████████▏| 98/107 [00:25<00:02,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Nashville-Davidson_metropolitan_government__balance__TN/2014-01/Merged_n36_w087_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  93%|█████████▎| 99/107 [00:26<00:02,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Tampa_FL/2014-01/Merged_n28_w083_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  94%|█████████▍| 101/107 [00:26<00:01,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Fayetteville_NC/2014-01/Merged_n34_w079_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  95%|█████████▌| 102/107 [00:27<00:01,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Albuquerque_NM/2014-01/Merged_n35_w107_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  98%|█████████▊| 105/107 [00:28<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Knoxville_TN/2014-01/Merged_n36_w085_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover:  99%|█████████▉| 106/107 [00:29<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging to ./Temp/RawRasters/DEM/Pahrump_NV/2014-01/Merged_n36_w116_1arc_v3.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging DEM and Land_Cover: 100%|██████████| 107/107 [00:30<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Merge all rasters to the first raster in the scene'''\n",
    "shapefile_folder = \"./Data/area_shp/\"\n",
    "cities_aoi = {} # Use cities dictionary instead of list to read from the raster itself\n",
    "for file in os.listdir(shapefile_folder):\n",
    "    if file.endswith(\".shp\"):\n",
    "        aoi_geodf = gpd.read_file(shapefile_folder + file)\n",
    "        aoi_geodf = aoi_geodf.to_crs(\"EPSG:4326\")\n",
    "        if aoi_geodf.empty:\n",
    "            sys.exit(\"Error: Shapefile contains no data.\")\n",
    "        cities_aoi[file.replace('Polygon_', '').replace('.shp', '')] = aoi_geodf\n",
    "\n",
    "import rasterio\n",
    "import rasterio.merge\n",
    "\n",
    "def merge_rasters(target_file, scene_files):\n",
    "    \"\"\"Merge multiple raster files into the first raster in the scene.\"\"\"\n",
    "    src_files_to_merge = []\n",
    "\n",
    "    for scene_file in scene_files:\n",
    "        src = rasterio.open(scene_file)\n",
    "        src_files_to_merge.append(src)\n",
    "\n",
    "    mosaic, out_trans = rasterio.merge.merge(src_files_to_merge)\n",
    "\n",
    "    out_meta = src_files_to_merge[0].meta.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": mosaic.shape[1],\n",
    "        \"width\": mosaic.shape[2],\n",
    "        \"transform\": out_trans\n",
    "    })\n",
    "\n",
    "    with rasterio.open(target_file, \"w\", **out_meta) as dest:\n",
    "        dest.write(mosaic)\n",
    "\n",
    "    for src in src_files_to_merge:\n",
    "        src.close()\n",
    "\n",
    "\n",
    "def checkRasterInPolygonAtAll(polygon: GeoDataFrame, ras: str) -> bool:\n",
    "    polygon = polygon.geometry.iloc[0]\n",
    "    with rasterio.open(ras) as src:\n",
    "        bounds = src.bounds\n",
    "        raster_bounds = Polygon([\n",
    "            (bounds.left, bounds.top),\n",
    "            (bounds.right, bounds.top),\n",
    "            (bounds.right, bounds.bottom),\n",
    "            (bounds.left, bounds.bottom),\n",
    "            (bounds.left, bounds.top)\n",
    "        ])\n",
    "\n",
    "    return polygon.intersects(raster_bounds)\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if\n",
    "             os.path.isfile(os.path.join(folder_path, f))]\n",
    "    return files\n",
    "notifySelf(\"Starting merge of DEM and Land_Cover...\")\n",
    "allGeoFiles = get_file_paths(raw_dir)\n",
    "allDEMLandCover, cityDatesTaken = [], []\n",
    "for geoFile in tqdm(allGeoFiles, desc=\"Filtering only 1st DEM,LC in scenes...\"):\n",
    "    geoFileParts = geoFile.split('/')\n",
    "    fileName, date, city, dataType = geoFileParts[-1], geoFileParts[-2], geoFileParts[-3], geoFileParts[-4]\n",
    "    polygon = cities_aoi[city]\n",
    "    if dataType == \"DEM\":              '''or dataType == \"Land_Cover\"'''\n",
    "        if city + \"-\" + date + \"-\" + dataType in cityDatesTaken:\n",
    "            continue\n",
    "        if checkRasterInPolygonAtAll(polygon, geoFile):\n",
    "            cityDatesTaken.append(city + \"-\" + date + \"-\" + dataType)\n",
    "            allDEMLandCover.append(geoFile)\n",
    "i, file25Percent = 0, int(len(allDEMLandCover)//4)\n",
    "for dem_lc_file in tqdm(allDEMLandCover, desc=\"Merging DEM and Land_Cover\"):\n",
    "    if i % file25Percent == 0:\n",
    "        percentage_done = int((i / len(allGeoFiles)) * 100)\n",
    "        notifySelf(f'We are at {percentage_done}% merged DEM LC')\n",
    "    i+=1\n",
    "    geoFileParts = dem_lc_file.split('/')\n",
    "    fileName, date, city, dataType = geoFileParts[-1], geoFileParts[-2], geoFileParts[-3], geoFileParts[-4]\n",
    "    targetFile = os.path.join(raw_dir, dataType, city, date, \"Merged_\" + fileName)\n",
    "    polygon = cities_aoi[city]\n",
    "    dem_lc_scene_files = list_files_in_folder(os.path.dirname(os.path.abspath(dem_lc_file)))\n",
    "    if len(dem_lc_scene_files) > 1:\n",
    "        print(\"Merging to \" + targetFile)\n",
    "        merge_rasters(targetFile, dem_lc_scene_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f6f4ab45b2bedc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T01:43:11.489953Z",
     "start_time": "2025-01-30T00:23:26.262830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make raster list/move txt files: 100%|██████████| 62027/62027 [1:19:42<00:00, 12.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "'''Clip Raster, we are reading metadata from rastername'''\n",
    "notifySelf(\"Starting clip list of raster paths...\")\n",
    "allGeoFiles = get_file_paths(raw_dir)\n",
    "i, file25Percent = 0, int(len(allGeoFiles)//4)\n",
    "usableTIFFs = []\n",
    "for geoFilePath in tqdm(allGeoFiles, desc=\"Make raster list/move txt files\"):\n",
    "    if i % file25Percent == 0:\n",
    "        percentage_done = int((i / len(allGeoFiles)) * 100)\n",
    "        notifySelf(f'We are at {percentage_done}% clipped')\n",
    "    i+=1\n",
    "    geoFileParts = geoFilePath.split('/')\n",
    "    fileName, date, city, dataType = geoFileParts[-1], geoFileParts[-2], geoFileParts[-3], geoFileParts[-4]\n",
    "    targetFile = os.path.join(clipped_dir, dataType, city, date, fileName)\n",
    "    if os.path.exists(targetFile):\n",
    "        continue\n",
    "    polygon = cities_aoi[city]\n",
    "    if geoFilePath.endswith(\".tif\") or geoFilePath.endswith(\".TIF\"):\n",
    "        if checkPolygonInRasterCompletely(polygon, geoFilePath):\n",
    "            usableTIFFs.append(geoFilePath)\n",
    "    elif geoFilePath.endswith(\".txt\"):\n",
    "        moveToClipped(geoFilePath, fileName, dataType, date, city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fb1c7c3c564ceed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T01:43:11.626283Z",
     "start_time": "2025-01-30T01:43:11.594274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving to log: 100%|██████████| 44/44 [00:00<00:00, 468399.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File paths saved to ./Logs/clipped_processed.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_paths_to_log(usableTIFFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da58d4336d201e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T01:44:27.212560Z",
     "start_time": "2025-01-30T01:43:11.639370Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping rasters: 100%|██████████| 58296/58296 [01:12<00:00, 800.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped and moved raw rasters successfully.\n"
     ]
    }
   ],
   "source": [
    "'''Clip and move Rasters'''\n",
    "filesList = read_file_paths_from_log()\n",
    "i, file25Percent = 0, int(len(filesList) // 4)\n",
    "notifySelf(\"Starting clip Rasters...\")\n",
    "#Clip, project and move to new folder.\n",
    "for geoFilePath in tqdm(filesList, desc=\"Clipping rasters\"):\n",
    "    if i % file25Percent == 0:\n",
    "        percentage_done = int((i / len(filesList)) * 100)\n",
    "        notifySelf(f'We are at {percentage_done}% clipped')\n",
    "    i += 1\n",
    "    geoFileParts = geoFilePath.split('/')\n",
    "    fileName, date, city, dataType = geoFileParts[-1], geoFileParts[-2], geoFileParts[-3], geoFileParts[-4]\n",
    "    target_folder = os.path.join(clipped_dir, dataType, city, date)\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    target_file = os.path.join(clipped_dir, dataType, city, date, fileName)\n",
    "    if os.path.exists(target_file):\n",
    "        continue\n",
    "    polygon = cities_aoi[city]\n",
    "    raster = rioxarray.open_rasterio(geoFilePath)\n",
    "    raster_reprojected = raster.rio.reproject(\"EPSG:4326\")\n",
    "    colormap = None\n",
    "    with rasterio.open(geoFilePath) as src:\n",
    "        if src.colorinterp[0] == rasterio.enums.ColorInterp.palette:\n",
    "            colormap = src.colormap(1)  # Assuming band 1 has the colormap\n",
    "    clipped = raster_reprojected.rio.clip(polygon.geometry, polygon.crs, drop=True)\n",
    "    clipped.rio.to_raster(target_file)\n",
    "    if colormap:\n",
    "        with rasterio.open(target_file, \"r+\") as dest:\n",
    "            dest.write_colormap(1, colormap)\n",
    "    raster.close()\n",
    "    raster_reprojected.close()\n",
    "print(\"Clipped and moved raw rasters successfully.\")\n",
    "notifySelf(\"Clipped and moved raw rasters successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e33b445900b0ef5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T03:26:36.773289Z",
     "start_time": "2025-02-01T01:44:15.774071Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Clouds & Missing Tifs: 100%|██████████| 24155/24155 [1:42:11<00:00,  3.94it/s]  \n"
     ]
    }
   ],
   "source": [
    "'''Categorize rasters by cloud coverage'''\n",
    "def moveToProcess(filePath: str, fileName: str, typeFolder: str, date: str, city: str, cloud_cover: str):\n",
    "    target_folder = os.path.join(process_dir, typeFolder, cloud_cover, city, date)\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    target_file_path = os.path.join(target_folder, fileName)\n",
    "    if os.path.exists(target_file_path):\n",
    "        return\n",
    "    shutil.copy2(filePath, target_file_path)\n",
    "\n",
    "notifySelf(\"Starting cloud removal...\")\n",
    "process_dir = temp_dir + '/Process'\n",
    "allClippedFiles, validCloudFiles, cloudRange = [], [], [5, 15, 25, 40, 60, 80, 100]\n",
    "for clippedFilePath in get_file_paths(clipped_dir):\n",
    "    if 'QA_PIXEL' in clippedFilePath:\n",
    "        allClippedFiles.append(clippedFilePath)\n",
    "i, file25Percent = 0, int(len(allClippedFiles) // 4)\n",
    "for clippedFilePath in tqdm(allClippedFiles, desc=\"Filtering Clouds & Missing Tifs\"):\n",
    "    if i % file25Percent == 0:\n",
    "        percentage_done = int((i / len(allClippedFiles)) * 100)\n",
    "        notifySelf(f'We are at {percentage_done}% clipped')\n",
    "    i += 1\n",
    "    cloudCover, cloudCategory = calculate_cloud_cover_percentage(clippedFilePath), 0\n",
    "    for coverPercentage in cloudRange:\n",
    "        if cloudCover <= coverPercentage:\n",
    "            cloudCategory = coverPercentage\n",
    "            break;\n",
    "    cloudCategory = f'less{cloudCategory}CloudCover'\n",
    "    fileParts = clippedFilePath.split('/')\n",
    "    fileName, date, city, dataType = fileParts[-1], fileParts[-2], fileParts[-3], fileParts[-4]\n",
    "    sceneIdentifyingName = '_'.join(fileName.split('_')[:7])\n",
    "    sceneFiles = []\n",
    "    for sceneFileAbsolutePath in list_files_in_folder(os.path.dirname(os.path.abspath(clippedFilePath))):\n",
    "        if sceneIdentifyingName in sceneFileAbsolutePath:\n",
    "            sceneFiles.append(sceneFileAbsolutePath)\n",
    "    if len(sceneFiles) == 10:\n",
    "        for sceneFile in sceneFiles:\n",
    "            moveToProcess(sceneFile, sceneFile.split('/')[-1], dataType, date, city, cloudCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4020c0bd811d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T05:56:06.288277Z",
     "start_time": "2025-02-03T04:17:08.342637Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/16284 [00:00<?, ?it/s]/work/ubh496/podman_storage/ipykernel_1916834/27504549.py:86: RuntimeWarning: invalid value encountered in divide\n",
      "  ndvi = np.where((nir + red) == 0, 0, (nir - red) / (nir + red))\n",
      "/work/ubh496/podman_storage/ipykernel_1916834/27504549.py:57: RuntimeWarning: invalid value encountered in divide\n",
      "  ndwi = np.where((green + nir) == 0, 0, (green - nir) / (green + nir))\n",
      "Processing: 100%|██████████| 16284/16284 [1:38:52<00:00,  2.74it/s] \n"
     ]
    }
   ],
   "source": [
    "'''Process indices'''\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if\n",
    "             os.path.isfile(os.path.join(folder_path, f))]\n",
    "    return files\n",
    "\n",
    "\n",
    "def extract_constants(mtl_file_path):\n",
    "    constants = {}\n",
    "    with open(mtl_file_path, 'r') as mtl_file:\n",
    "        lines = mtl_file.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"RADIANCE_MULT_BAND_10\"):\n",
    "            constants['ML'] = float(line.split(\" = \")[1])\n",
    "        elif line.startswith(\"TEMPERATURE_ADD_BAND_ST_B10\"):\n",
    "            constants['ST_OFFSET'] = float(line.split(\" = \")[1])\n",
    "        elif line.startswith(\"TEMPERATURE_MULT_BAND_ST_B10\"):\n",
    "            constants['ST_MULT'] = float(line.split(\" = \")[1])\n",
    "        elif line.startswith(\"RADIANCE_ADD_BAND_10\"):\n",
    "            constants['AL'] = float(line.split(\" = \")[1])\n",
    "        elif line.startswith(\"K1_CONSTANT_BAND_10\"):\n",
    "            constants['K1'] = float(line.split(\" = \")[1])\n",
    "        elif line.startswith(\"K2_CONSTANT_BAND_10\"):\n",
    "            constants['K2'] = float(line.split(\" = \")[1])\n",
    "        for band in range(2, 7):  # Loop through bands 2 to 6\n",
    "            mult_key = f\"REFLECTANCE_MULT_BAND_{band}\"\n",
    "            add_key = f\"REFLECTANCE_ADD_BAND_{band}\"\n",
    "            if line.startswith(mult_key):\n",
    "                constants[mult_key] = float(line.split(\" = \")[1])\n",
    "            elif line.startswith(add_key):\n",
    "                constants[add_key] = float(line.split(\" = \")[1])\n",
    "    return constants\n",
    "\n",
    "\n",
    "def retrieve_ndwi(green_band_path, nir_band_path, reflectance_mult: dict, reflectance_add: dict, output_path: str):\n",
    "    with rasterio.open(green_band_path) as green_src, rasterio.open(nir_band_path) as nir_src, \\\n",
    "         rasterio.open(cloud) as cloud_src:\n",
    "        green = green_src.read(1).astype('float32') * reflectance_mult['B3'] + reflectance_add['B3']\n",
    "        nir = nir_src.read(1).astype('float32') * reflectance_mult['B5'] + reflectance_add['B5']\n",
    "        qa_pixel = cloud_src.read(1).astype('uint16')\n",
    "        cloud_nodata_value = cloud_src.nodata\n",
    "        nodata_value = nir_src.nodata\n",
    "        if cloud_nodata_value is not None:\n",
    "            cloud_nodata_mask = (qa_pixel == cloud_nodata_value)\n",
    "        else:\n",
    "            cloud_nodata_mask = np.zeros_like(qa_pixel, dtype=bool)\n",
    "        cloud_confidence_mask = (qa_pixel & 0b00011000) >> 3\n",
    "        cloud_shadow_mask = (qa_pixel & 0b00100000) >> 5\n",
    "        cloud_pixels = (cloud_confidence_mask >= 1) | (cloud_shadow_mask == 1)\n",
    "        valid_pixels = ~cloud_nodata_mask\n",
    "        cloud_mask = cloud_pixels & valid_pixels\n",
    "        green = np.clip(green, 0, 1)\n",
    "        nir = np.clip(nir, 0, 1)\n",
    "\n",
    "        ndwi = np.where((green + nir) == 0, 0, (green - nir) / (green + nir))\n",
    "        ndwi[cloud_mask] = nodata_value\n",
    "\n",
    "        profile = green_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1)\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(ndwi, 1)\n",
    "\n",
    "\n",
    "def retrieve_ndvi(nir_band_path, red_band_path, cloud, reflectance_mult: dict, reflectance_add: dict, output_path):\n",
    "    with rasterio.open(nir_band_path) as nir_src, rasterio.open(red_band_path) as red_src, \\\n",
    "         rasterio.open(cloud) as cloud_src:\n",
    "        nir = nir_src.read(1).astype('float32') * reflectance_mult['B5'] + reflectance_add['B5']\n",
    "        red = red_src.read(1).astype('float32') * reflectance_mult['B4'] + reflectance_add['B4']\n",
    "        qa_pixel = cloud_src.read(1).astype('uint16')\n",
    "        cloud_nodata_value = cloud_src.nodata\n",
    "        nodata_value = red_src.nodata\n",
    "        if cloud_nodata_value is not None:\n",
    "            cloud_nodata_mask = (qa_pixel == cloud_nodata_value)\n",
    "        else:\n",
    "            cloud_nodata_mask = np.zeros_like(qa_pixel, dtype=bool)\n",
    "        cloud_confidence_mask = (qa_pixel & 0b00011000) >> 3\n",
    "        cloud_shadow_mask = (qa_pixel & 0b00100000) >> 5\n",
    "        cloud_pixels = (cloud_confidence_mask >= 1) | (cloud_shadow_mask == 1)\n",
    "        valid_pixels = ~cloud_nodata_mask\n",
    "        cloud_mask = cloud_pixels & valid_pixels\n",
    "        nir = np.clip(nir, 0, 1)\n",
    "        red = np.clip(red, 0, 1)\n",
    "        ndvi = np.where((nir + red) == 0, 0, (nir - red) / (nir + red))\n",
    "        ndvi[cloud_mask] = nodata_value\n",
    "        profile = nir_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1)\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(ndvi, 1)\n",
    "\n",
    "def retrieve_ndbi(swir1_band_path, nir_band_path, cloud, reflectance_mult: dict, reflectance_add: dict, output_path):\n",
    "    with rasterio.open(swir1_band_path) as swir1_src, rasterio.open(nir_band_path) as nir_src, \\\n",
    "         rasterio.open(cloud) as cloud_src:\n",
    "        # Read bands and apply reflectance scaling\n",
    "        swir1 = swir1_src.read(1).astype('float32') * reflectance_mult['B6'] + reflectance_add['B6']\n",
    "        nir = nir_src.read(1).astype('float32') * reflectance_mult['B5'] + reflectance_add['B5']\n",
    "        \n",
    "        # Read QA pixel data for cloud masking\n",
    "        qa_pixel = cloud_src.read(1).astype('uint16')\n",
    "        cloud_nodata_value = cloud_src.nodata\n",
    "        nodata_value = nir_src.nodata\n",
    "        \n",
    "        # Create cloud mask\n",
    "        if cloud_nodata_value is not None:\n",
    "            cloud_nodata_mask = (qa_pixel == cloud_nodata_value)\n",
    "        else:\n",
    "            cloud_nodata_mask = np.zeros_like(qa_pixel, dtype=bool)\n",
    "            \n",
    "        # Extract cloud confidence and cloud shadow bits\n",
    "        cloud_confidence_mask = (qa_pixel & 0b00011000) >> 3\n",
    "        cloud_shadow_mask = (qa_pixel & 0b00100000) >> 5\n",
    "        \n",
    "        # A pixel is flagged as cloudy if cloud confidence is >= 1 or if cloud shadow bit is set\n",
    "        cloud_pixels = (cloud_confidence_mask >= 1) | (cloud_shadow_mask == 1)\n",
    "        \n",
    "        # Combine with valid pixels\n",
    "        valid_pixels = ~cloud_nodata_mask\n",
    "        cloud_mask = cloud_pixels & valid_pixels\n",
    "        \n",
    "        # Clip reflectance values to valid range [0,1]\n",
    "        swir1 = np.clip(swir1, 0, 1)\n",
    "        nir = np.clip(nir, 0, 1)\n",
    "        \n",
    "        # Calculate NDBI\n",
    "        ndbi = np.where((swir1 + nir) == 0, 0, (swir1 - nir) / (swir1 + nir))\n",
    "        \n",
    "        # Apply cloud mask\n",
    "        ndbi[cloud_mask] = nodata_value\n",
    "        \n",
    "        # Update profile for output\n",
    "        profile = swir1_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1)\n",
    "        \n",
    "        # Write the resulting NDBI raster to disk\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(ndbi, 1)\n",
    "\n",
    "def retrieve_lst(band10_path, cloud, constants, output_path):\n",
    "    with rasterio.open(band10_path) as band10_src, \\\n",
    "         rasterio.open(cloud) as cloud_src:\n",
    "\n",
    "        # Read raster data\n",
    "        q_cal = band10_src.read(1).astype('float32')\n",
    "        qa_pixel = cloud_src.read(1).astype('uint16')\n",
    "\n",
    "        # Get nodata values\n",
    "        band10_nodata = band10_src.nodata\n",
    "        cloud_nodata_value = cloud_src.nodata  # Get NoData value from raster metadata\n",
    "\n",
    "        # Valid mask to exclude nodata values\n",
    "        valid_mask = (q_cal != band10_nodata)\n",
    "\n",
    "        if cloud_nodata_value is not None:\n",
    "            cloud_nodata_mask = (qa_pixel == cloud_nodata_value)\n",
    "        else:\n",
    "            cloud_nodata_mask = np.zeros_like(qa_pixel, dtype=bool)\n",
    "\n",
    "        cloud_confidence_mask = (qa_pixel & 0b00011000) >> 3\n",
    "        cloud_shadow_mask = (qa_pixel & 0b00100000) >> 5\n",
    "        cloud_pixels = (cloud_confidence_mask >= 1) | (cloud_shadow_mask == 1)\n",
    "\n",
    "        valid_pixels = ~cloud_nodata_mask\n",
    "        cloud_mask = cloud_pixels & valid_pixels\n",
    "\n",
    "        valid_mask = valid_mask & ~cloud_mask\n",
    "\n",
    "        # Apply scale factor and offset to get temperature in Kelvin\n",
    "        T_K = (q_cal * constants['ST_MULT']) + constants['ST_OFFSET']\n",
    "\n",
    "        # Convert Kelvin to Fahrenheit\n",
    "        lst_fahrenheit = np.where(valid_mask, (T_K - 273.15) * 1.8 + 32, np.nan)\n",
    "\n",
    "        # Save the new raster\n",
    "        with rasterio.open(\n",
    "            output_path,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=band10_src.height,\n",
    "            width=band10_src.width,\n",
    "            count=1,\n",
    "            dtype=lst_fahrenheit.dtype,\n",
    "            crs=band10_src.crs,\n",
    "            transform=band10_src.transform,\n",
    "            nodata=np.nan\n",
    "        ) as dst:\n",
    "            dst.write(lst_fahrenheit, 1)\n",
    "\n",
    "    # print(f\"Converted LST saved to: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_albedo_all(band2_path, band3_path, band4_path, band5_path, band6_path, qa_path,\n",
    "                        reflectance_mult: dict, reflectance_add: dict, output_path):\n",
    "    coefficients = {\n",
    "        \"band2\": 0.356,\n",
    "        \"band3\": 0.130,\n",
    "        \"band4\": 0.373,\n",
    "        \"band5\": 0.085,\n",
    "        \"band6\": 0.072,\n",
    "        \"offset\": -0.018\n",
    "    }\n",
    "\n",
    "    with rasterio.open(band2_path) as band2_src, \\\n",
    "         rasterio.open(band3_path) as band3_src, \\\n",
    "         rasterio.open(band4_path) as band4_src, \\\n",
    "         rasterio.open(band5_path) as band5_src, \\\n",
    "         rasterio.open(band6_path) as band6_src, \\\n",
    "         rasterio.open(qa_path) as cloud_src:  # QA band is opened but not used here\n",
    "\n",
    "        # Read raw bands (before applying scaling)\n",
    "        raw_band2 = band2_src.read(1).astype('float32')\n",
    "        raw_band3 = band3_src.read(1).astype('float32')\n",
    "        raw_band4 = band4_src.read(1).astype('float32')\n",
    "        raw_band5 = band5_src.read(1).astype('float32')\n",
    "        raw_band6 = band6_src.read(1).astype('float32')\n",
    "        qa_pixel = cloud_src.read(1).astype('uint16')\n",
    "\n",
    "        # Get the nodata value (assumes the same nodata for all bands)\n",
    "        nodata_value = band2_src.nodata\n",
    "        cloud_nodata_value = cloud_src.nodata\n",
    "\n",
    "        # Create a nodata mask from the raw bands\n",
    "        nodata_mask = ((raw_band2 == nodata_value) |\n",
    "                       (raw_band3 == nodata_value) |\n",
    "                       (raw_band4 == nodata_value) |\n",
    "                       (raw_band5 == nodata_value) |\n",
    "                       (raw_band6 == nodata_value))\n",
    "\n",
    "        # Apply scaling to each band using the provided reflectance multipliers and offsets\n",
    "        band2 = raw_band2 * reflectance_mult['B2'] + reflectance_add['B2']\n",
    "        band3 = raw_band3 * reflectance_mult['B3'] + reflectance_add['B3']\n",
    "        band4 = raw_band4 * reflectance_mult['B4'] + reflectance_add['B4']\n",
    "        band5 = raw_band5 * reflectance_mult['B5'] + reflectance_add['B5']\n",
    "        band6 = raw_band6 * reflectance_mult['B6'] + reflectance_add['B6']\n",
    "\n",
    "        # Compute albedo using the NTB coefficients (clip each band between 0 and 1)\n",
    "        albedo = (\n",
    "            coefficients[\"band2\"] * np.clip(band2, 0, 1) +\n",
    "            coefficients[\"band3\"] * np.clip(band3, 0, 1) +\n",
    "            coefficients[\"band4\"] * np.clip(band4, 0, 1) +\n",
    "            coefficients[\"band5\"] * np.clip(band5, 0, 1) +\n",
    "            coefficients[\"band6\"] * np.clip(band6, 0, 1) +\n",
    "            coefficients[\"offset\"]\n",
    "        )\n",
    "\n",
    "        if cloud_nodata_value is not None:\n",
    "            cloud_nodata_mask = (qa_pixel == cloud_nodata_value)\n",
    "        else:\n",
    "            cloud_nodata_mask = np.zeros_like(qa_pixel, dtype=bool)\n",
    "        cloud_confidence_mask = (qa_pixel & 0b00011000) >> 3\n",
    "        cloud_shadow_mask = (qa_pixel & 0b00100000) >> 5\n",
    "        cloud_pixels = (cloud_confidence_mask >= 1) | (cloud_shadow_mask == 1)\n",
    "\n",
    "        valid_pixels = ~cloud_nodata_mask\n",
    "        cloud_mask = cloud_pixels & valid_pixels\n",
    "\n",
    "        # Apply the nodata mask to the computed albedo: any pixel with nodata in any band is set to nodata.\n",
    "        albedo[nodata_mask | cloud_mask] = nodata_value\n",
    "\n",
    "\n",
    "        # Update the output profile to include the nodata setting and ensure the dtype is correct\n",
    "        profile = band2_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1, nodata=nodata_value)\n",
    "\n",
    "        # Write the resulting albedo raster to disk\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(albedo, 1)\n",
    "\n",
    "def retrieve_albedo(band2_path, band3_path, band4_path, band5_path, band6_path, qa_path,\n",
    "                    reflectance_mult: dict, reflectance_add: dict, output_path):\n",
    "    \"\"\"\n",
    "    Compute surface albedo from Landsat 8 bands and exclude cloudy pixels following\n",
    "    the logic used in calculate_cloud_cover_percentage.\n",
    "\n",
    "    The albedo is computed as:\n",
    "\n",
    "    albedo = c2 * clip(B2, 0, 1) + c3 * clip(B3, 0, 1) + c4 * clip(B4, 0, 1) +\n",
    "             c5 * clip(B5, 0, 1) + c6 * clip(B6, 0, 1) + offset\n",
    "\n",
    "    In LaTeX:\n",
    "\n",
    "    ```latex\n",
    "    \\[\n",
    "      \\text{albedo} = 0.356 \\cdot \\text{clip}(B_2, 0, 1)\n",
    "                    + 0.130 \\cdot \\text{clip}(B_3, 0, 1)\n",
    "                    + 0.373 \\cdot \\text{clip}(B_4, 0, 1)\n",
    "                    + 0.085 \\cdot \\text{clip}(B_5, 0, 1)\n",
    "                    + 0.072 \\cdot \\text{clip}(B_6, 0, 1)\n",
    "                    - 0.018\n",
    "    \\]\n",
    "    ```\n",
    "\n",
    "    Cloudy pixels are determined by:\n",
    "      - cloud_confidence_mask = (QA_PIXEL & 0b00011000) >> 3\n",
    "      - cloud_shadow_mask   = (QA_PIXEL & 0b00100000) >> 5\n",
    "\n",
    "    A pixel is flagged as cloudy if:\n",
    "\n",
    "    ```python\n",
    "    cloud_mask = (cloud_confidence_mask >= 1) | (cloud_shadow_mask == 1)\n",
    "    ```\n",
    "\n",
    "    These cloudy pixels, along with any pixels already marked as nodata,\n",
    "    are then set to the nodata value in the final albedo raster.\n",
    "    \"\"\"\n",
    "    # NTB coefficients for Landsat 8 bands\n",
    "    coefficients = {\n",
    "        \"band2\": 0.356,\n",
    "        \"band3\": 0.130,\n",
    "        \"band4\": 0.373,\n",
    "        \"band5\": 0.085,\n",
    "        \"band6\": 0.072,\n",
    "        \"offset\": -0.018\n",
    "    }\n",
    "\n",
    "    with rasterio.open(band2_path) as band2_src, \\\n",
    "         rasterio.open(band3_path) as band3_src, \\\n",
    "         rasterio.open(band4_path) as band4_src, \\\n",
    "         rasterio.open(band5_path) as band5_src, \\\n",
    "         rasterio.open(band6_path) as band6_src, \\\n",
    "         rasterio.open(qa_path) as qa_src:\n",
    "\n",
    "        # Read raw bands (before applying scaling)\n",
    "        raw_band2 = band2_src.read(1).astype('float32')\n",
    "        raw_band3 = band3_src.read(1).astype('float32')\n",
    "        raw_band4 = band4_src.read(1).astype('float32')\n",
    "        raw_band5 = band5_src.read(1).astype('float32')\n",
    "        raw_band6 = band6_src.read(1).astype('float32')\n",
    "\n",
    "        # Get the nodata value (assumes the same nodata for all bands)\n",
    "        nodata_value = band2_src.nodata\n",
    "\n",
    "        # Create a nodata mask from the raw bands\n",
    "        nodata_mask = ((raw_band2 == nodata_value) |\n",
    "                       (raw_band3 == nodata_value) |\n",
    "                       (raw_band4 == nodata_value) |\n",
    "                       (raw_band5 == nodata_value) |\n",
    "                       (raw_band6 == nodata_value))\n",
    "\n",
    "        # Apply scaling to each band using the provided reflectance multipliers and offsets\n",
    "        band2 = raw_band2 * reflectance_mult['B2'] + reflectance_add['B2']\n",
    "        band3 = raw_band3 * reflectance_mult['B3'] + reflectance_add['B3']\n",
    "        band4 = raw_band4 * reflectance_mult['B4'] + reflectance_add['B4']\n",
    "        band5 = raw_band5 * reflectance_mult['B5'] + reflectance_add['B5']\n",
    "        band6 = raw_band6 * reflectance_mult['B6'] + reflectance_add['B6']\n",
    "\n",
    "        # Compute albedo using the NTB coefficients (clipping values between 0 and 1)\n",
    "        albedo = (\n",
    "            coefficients[\"band2\"] * np.clip(band2, 0, 1) +\n",
    "            coefficients[\"band3\"] * np.clip(band3, 0, 1) +\n",
    "            coefficients[\"band4\"] * np.clip(band4, 0, 1) +\n",
    "            coefficients[\"band5\"] * np.clip(band5, 0, 1) +\n",
    "            coefficients[\"band6\"] * np.clip(band6, 0, 1) +\n",
    "            coefficients[\"offset\"]\n",
    "        )\n",
    "\n",
    "        # --- Cloud Masking using the QA_PIXEL band ---\n",
    "        # Read the QA_PIXEL band\n",
    "        qa = qa_src.read(1)\n",
    "\n",
    "        # Extract cloud confidence and cloud shadow bits using the same logic as in\n",
    "        # calculate_cloud_cover_percentage\n",
    "        cloud_confidence_mask = (qa & 0b00011000) >> 3\n",
    "        cloud_shadow_mask = (qa & 0b00100000) >> 5\n",
    "\n",
    "        # A pixel is flagged as cloudy if cloud confidence is >= 1 or if cloud shadow bit is set\n",
    "        cloud_mask = (cloud_confidence_mask >= 1) | (cloud_shadow_mask == 1)\n",
    "\n",
    "        # Combine the original nodata mask with the cloud mask\n",
    "        final_mask = nodata_mask | cloud_mask\n",
    "\n",
    "        # Set albedo to nodata_value for pixels flagged as nodata or cloudy\n",
    "        albedo[final_mask] = nodata_value\n",
    "\n",
    "        # Update the output profile, ensuring nodata is preserved and the dtype is correct\n",
    "        profile = band2_src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1, nodata=nodata_value)\n",
    "\n",
    "        # Write the resulting albedo raster to disk\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(albedo, 1)\n",
    "\n",
    "\n",
    "def getDataPath(fileName, typeFolder: str, date, city, cloud_cover):\n",
    "    target_folder = os.path.join(data_dir, typeFolder, cloud_cover, city, date)\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    target_file_path = os.path.join(target_folder, fileName)\n",
    "    return target_file_path\n",
    "\n",
    "\n",
    "def getClippedPath(typeFolder: str, date, city):\n",
    "    target_folder = os.path.join(clipped_dir, typeFolder, city, date)\n",
    "    if os.path.exists(target_folder):\n",
    "        target_files = os.listdir(target_folder)\n",
    "        if len(target_files) > 0:\n",
    "            targetFile = os.listdir(target_folder)[0]\n",
    "            return os.path.join(target_folder, targetFile)\n",
    "    return 'invalid'\n",
    "\n",
    "def raster_statistics(raster_path):\n",
    "    with rasterio.open(raster_path) as dataset:\n",
    "        band1 = dataset.read(1)  # Read the first band\n",
    "\n",
    "        # Mask out no-data values if they exist\n",
    "        if dataset.nodata is not None:\n",
    "            valid_mask = band1 != dataset.nodata\n",
    "            band1 = band1[valid_mask]\n",
    "\n",
    "        min_val = np.min(band1)\n",
    "        max_val = np.max(band1)\n",
    "        mean_val = np.mean(band1)\n",
    "        print(band1.meta)\n",
    "        print(band1.scales, src.offsets)\n",
    "        print(f'Band10 Mean value: {mean_val}, max: {max_val}, min: {min_val}')\n",
    "\n",
    "def create_heat_index(lst_path, output_path):\n",
    "    with rasterio.open(lst_path) as src:\n",
    "        lst = src.read(1).astype('float32')  # Ensure float32 for NaN support\n",
    "        nodata_value = src.nodata  # Get original NoData value (NaN or a specific number)\n",
    "\n",
    "        # Identify valid pixels\n",
    "        valid_mask = ~np.isnan(lst) if np.isnan(nodata_value) else lst != nodata_value\n",
    "        valid_lst = lst[valid_mask]\n",
    "\n",
    "        # If there are no valid pixels, raise an error\n",
    "        if valid_lst.size == 0:\n",
    "            raise ValueError(\"No valid LST data found. Check input raster.\")\n",
    "\n",
    "        # Determine min and max for valid LST values\n",
    "        lst_min, lst_max = np.min(valid_lst), np.max(valid_lst)\n",
    "\n",
    "        # Define 10 categories for the heat index\n",
    "        category_bounds = np.linspace(lst_min, lst_max, 11)\n",
    "\n",
    "        # Initialize the heat index raster with NaN (to preserve NoData)\n",
    "        lst_categories = np.full_like(lst, np.nan, dtype='float32')\n",
    "\n",
    "        # Assign categories only to valid pixels\n",
    "        lst_categories[valid_mask] = np.digitize(valid_lst, category_bounds, right=True)\n",
    "\n",
    "        # Preserve the profile and update metadata\n",
    "        profile = src.profile\n",
    "        profile.update(dtype=rasterio.float32, count=1, nodata=nodata_value)  # Keep nodata consistent\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(lst_categories, 1)\n",
    "\n",
    "def get_file_paths(folder_path):\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.abspath(os.path.join(root, file))\n",
    "            file_paths.append(full_path)\n",
    "    return file_paths\n",
    "\n",
    "allQAPixelFiles = []\n",
    "for filePath in get_file_paths(process_dir):\n",
    "    if 'QA_PIXEL' in filePath:\n",
    "        allQAPixelFiles.append(filePath)\n",
    "for filePath in tqdm(allQAPixelFiles, desc=\"Processing\"):\n",
    "    fileParts = filePath.split('/')\n",
    "    fileName, date, city, cloudCategory, dataType = fileParts[-1], fileParts[-2], fileParts[-3], fileParts[-4], fileParts[-5]\n",
    "    sceneIdentifyingName = '_'.join(fileName.split('_')[:7])\n",
    "    sceneFiles = []\n",
    "    for sceneFileAbsolutePath in list_files_in_folder(os.path.dirname(os.path.abspath(filePath))):\n",
    "        if sceneIdentifyingName in sceneFileAbsolutePath:\n",
    "            sceneFiles.append(sceneFileAbsolutePath)\n",
    "    band_files = {}\n",
    "\n",
    "    for sceneFile in sceneFiles:\n",
    "        band = sceneFile.split('_')[-1].replace('.TIF', '').replace('.txt', '').replace('.tif', '')\n",
    "        band_files[band] = sceneFile  # Store all band files in a dictionary\n",
    "\n",
    "    for sceneFile in sceneFiles:\n",
    "        band = sceneFile.split('_')[-1].replace('.TIF', '').replace('.txt', '').replace('.tif', '')\n",
    "        if band == 'MTL':\n",
    "            MTL = sceneFile\n",
    "        if band == 'PIXEL':\n",
    "            cloud = sceneFile\n",
    "    constants = extract_constants(MTL)\n",
    "\n",
    "    for sceneFile in sceneFiles:\n",
    "        band = sceneFile.split('_')[-1].replace('.TIF', '').replace('.txt', '').replace('.tif', '')\n",
    "        if band == 'MTL':\n",
    "            MTL = sceneFile\n",
    "        if band == 'PIXEL':\n",
    "            cloud = sceneFile\n",
    "        if band == 'B10':\n",
    "            band10 = sceneFile\n",
    "        if band == 'B2':\n",
    "            band2 = sceneFile\n",
    "        if band == 'B3':\n",
    "            band3 = sceneFile\n",
    "        if band == 'B4':\n",
    "            band4 = sceneFile\n",
    "        if band == 'B5':\n",
    "            band5 = sceneFile\n",
    "        if band == 'B6':\n",
    "            band6 = sceneFile\n",
    "        if band == 'B7':\n",
    "            band7 = sceneFile\n",
    "        if band == 'EMIS':\n",
    "            emis = sceneFile\n",
    "    constants = extract_constants(MTL)\n",
    "    reflectance_mult, reflectance_add = {}, {}\n",
    "    bands = [\"B2\", \"B3\", \"B4\", \"B5\", \"B6\"]\n",
    "    for band in bands:\n",
    "        reflectance_mult[band] = constants[f\"REFLECTANCE_MULT_BAND_{band.replace('B', '')}\"]\n",
    "        reflectance_add[band] = constants[f\"REFLECTANCE_ADD_BAND_{band.replace('B', '')}\"]\n",
    "    # raster_statistics(band10)\n",
    "    # print(calculate_cloud_cover_percentage(cloud))\n",
    "    demPath = getClippedPath('DEM', '2014-01', city)\n",
    "    landCover = getClippedPath('Land_Cover', date.split('-')[0] + \"-01\", city)\n",
    "    if os.path.exists(demPath) and os.path.exists(landCover):\n",
    "        # shutil.copy2(demPath, getDataPath('DEM.tif', 'X', date, city, cloudCategory))\n",
    "        # shutil.copy2(landCover, getDataPath('Land_Cover.tif', 'X', date, city, cloudCategory))\n",
    "        retrieve_ndbi(band6, band5, cloud, reflectance_mult, reflectance_add, getDataPath('NDBI.tif', 'X', date, city, cloudCategory))\n",
    "        # retrieve_ndvi(band5, band4, cloud, reflectance_mult, reflectance_add, getDataPath('NDVI.tif', 'X', date, city, cloudCategory))\n",
    "        # retrieve_ndwi(band3, band5, reflectance_mult, reflectance_add, getDataPath('NDWI.tif', 'X', date, city, cloudCategory))\n",
    "        # retrieve_albedo_all(band2, band3, band4, band5, band6, cloud, reflectance_mult, reflectance_add, getDataPath('Albedo.tif', 'X', date, city, cloudCategory))\n",
    "        # retrieve_lst(band10, cloud, constants, getDataPath('LST.tif', 'y', date, city, cloudCategory))\n",
    "        # create_heat_index(getDataPath('LST.tif', 'y', date, city, cloudCategory), getDataPath('Heat_Index.tif', 'y', date, city, cloudCategory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd10902a2d0029c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T15:32:59.711490Z",
     "start_time": "2025-02-03T15:32:59.696509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albedo.tif 33.22878972943077 33.22878972943113\n",
      "DEM.tif 30.922222222220395 30.922222222222047\n",
      "Land_Cover.tif 33.077629425293466 33.077629425293836\n",
      "NDVI.tif 33.22878972943077 33.22878972943113\n",
      "NDWI.tif 33.22878972943077 33.22878972943113\n",
      "LST.tif 33.22878972943077 33.22878972943113\n"
     ]
    }
   ],
   "source": [
    "deg_to_m = 111320\n",
    "for raster_path in [\"/work/ubh496/heat-island-test/Data/X/less5CloudCover/Abilene_TX/2013-06/Albedo.tif\", \"/work/ubh496/heat-island-test/Data/X/less5CloudCover/Abilene_TX/2013-06/DEM.tif\", \"/work/ubh496/heat-island-test/Data/X/less5CloudCover/Abilene_TX/2013-06/Land_Cover.tif\", \"/work/ubh496/heat-island-test/Data/X/less5CloudCover/Abilene_TX/2013-06/NDVI.tif\", \"/work/ubh496/heat-island-test/Data/X/less5CloudCover/Abilene_TX/2013-06/NDWI.tif\", \"/work/ubh496/heat-island-test/Data/y/less5CloudCover/Abilene_TX/2013-06/LST.tif\"]:\n",
    "    with rasterio.open(raster_path) as dataset:\n",
    "        pixel_width_deg, pixel_height_deg = dataset.res\n",
    "        pixel_width_m = pixel_width_deg * deg_to_m\n",
    "        pixel_height_m = pixel_height_deg * deg_to_m\n",
    "        print(raster_path.split('/')[-1] + \" \" + str(pixel_width_m) + \" \" + str(pixel_height_m))  # (pixel width, pixel height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22e35b5e9b8dd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T04:46:04.554570997Z",
     "start_time": "2025-01-22T02:08:40.584920Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db027b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import os\n",
    "from rasterio.windows import Window\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42687558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data.TiledLandsatDataModule import TiledGeotiffDataset\n",
    "import glob\n",
    "\n",
    "def combine_scene_tifs(scene_id_folder):\n",
    "    \"\"\"Combine LST and HeatIndex TIF files using rasterio.merge.\"\"\"\n",
    "    import rasterio\n",
    "    from rasterio.merge import merge\n",
    "    import glob\n",
    "    import os\n",
    "    \n",
    "    os.makedirs(scene_id_folder, exist_ok=True)\n",
    "    \n",
    "    for outTif in ['LST.tif', 'HeatIndex.tif']:\n",
    "        tif_files = glob.glob(os.path.join(scene_id_folder, f'predicted_*_{outTif}'))\n",
    "        print(f\"Found {len(tif_files)} files for {outTif}\")\n",
    "        print(tif_files)\n",
    "        \n",
    "        if not tif_files:\n",
    "            print(f\"No {outTif} files found in {scene_id_folder}\")\n",
    "            continue\n",
    "            \n",
    "        # Open all files\n",
    "        src_files = [rasterio.open(f) for f in tif_files]\n",
    "        \n",
    "        # Merge them\n",
    "        mosaic, out_transform = merge(src_files, nodata=-9999)\n",
    "        \n",
    "        # Get metadata from first file\n",
    "        profile = src_files[0].profile.copy()\n",
    "        profile.update({\n",
    "            \"height\": mosaic.shape[1],\n",
    "            \"width\": mosaic.shape[2],\n",
    "            \"transform\": out_transform,\n",
    "            \"nodata\": -9999\n",
    "        })\n",
    "        \n",
    "        # Save merged result\n",
    "        output_file = os.path.join(os.path.dirname(scene_id_folder), \n",
    "                                  f\"{os.path.basename(scene_id_folder)}_combined_{outTif}\")\n",
    "        with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "            dst.write(mosaic)\n",
    "            \n",
    "        print(f\"Combined {outTif} saved as: {output_file}\")\n",
    "        \n",
    "        # Close the open files\n",
    "        for src in src_files:\n",
    "            src.close()\n",
    "\n",
    "def inferenceCity(city: str, model, test_loader, device='cuda', denormalize: bool=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()    \n",
    "    batch = 0\n",
    "    it = iter(test_loader)\n",
    "    scenes = set()\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(len(test_loader)), desc=f\"Processing {city} tiles...\"):\n",
    "            \n",
    "            # Get one sample\n",
    "            sample = next(it)\n",
    "            ground_truth_file = sample['file_dict']['LST.tif'][0]\n",
    "            if city not in ground_truth_file:\n",
    "                continue\n",
    "            sleep(1)\n",
    "            for l, outTif in enumerate(['LST.tif', 'HeatIndex.tif']):\n",
    "                ground_truth_file = sample['file_dict'][outTif][0]\n",
    "                if city not in ground_truth_file:\n",
    "                    continue\n",
    "                inputs = sample['input'].to(device)\n",
    "                targets = sample['target'].to(device)\n",
    "                mask = sample['mask'].to(device)\n",
    "                ground_truth_file = sample['file_dict'][outTif][0]\n",
    "                box = sample['box']\n",
    "                box = [int(tensor.item()) for tensor in box]\n",
    "\n",
    "                # Get model prediction\n",
    "                outputs = model(inputs)\n",
    "                if denormalize:\n",
    "                    # print(outputs.shape)\n",
    "                    outputs = TiledGeotiffDataset.denormalize(outputs)[batch][l]\n",
    "                    targets = TiledGeotiffDataset.denormalize(targets)[batch][l]\n",
    "\n",
    "                # Move to CPU and convert to numpy\n",
    "                mask_np = mask.cpu().numpy().squeeze()\n",
    "                targets_np = targets.cpu().numpy().squeeze()\n",
    "                predicted_np = outputs.cpu().numpy().squeeze()\n",
    "                \n",
    "                # Apply mask\n",
    "                predicted_np[~mask_np] = -9999\n",
    "                targets_np[~mask_np] = -9999\n",
    "\n",
    "                output_dir = \"./Data/prediction\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                window = Window(col_off=xmin, row_off=ymin, width=xmax-xmin, height=ymax-ymin)\n",
    "                \n",
    "                # Get corresponding LST file path and save outputs\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                with rasterio.open(ground_truth_file) as src:\n",
    "                    profile = src.profile.copy()\n",
    "                    # Update the transform based on the window\n",
    "                    window_transform = rasterio.windows.transform(window, src.transform)\n",
    "                    \n",
    "                    # Update profile with new dimensions and transform\n",
    "                    profile.update(\n",
    "                        width=xmax-xmin,\n",
    "                        height=ymax-ymin,\n",
    "                        transform=window_transform,\n",
    "                        count=1,\n",
    "                        nodata=-9999\n",
    "                    )                                                    \n",
    "                    \n",
    "                    # Save prediction\n",
    "                    scene_id = f\"{ground_truth_file.split('/')[-3]}_{ground_truth_file.split('/')[-2]}\"  # city_date\n",
    "                    scenes.add(os.path.join(output_dir, scene_id))\n",
    "                    os.makedirs(os.path.join(output_dir, scene_id), exist_ok=True)\n",
    "                    pred_filename = os.path.join(output_dir, scene_id, f'predicted_{timestamp}_{outTif}')\n",
    "                    with rasterio.open(pred_filename, \"w\", **profile) as dst:\n",
    "                        dst.write(predicted_np.astype(np.float32), 1)\n",
    "                    \n",
    "                    # Save ground truth\n",
    "                    os.makedirs(os.path.join(output_dir, f'ground_truth_{scene_id}'), exist_ok=True)\n",
    "                    truth_filename = os.path.join(output_dir, f'ground_truth_{scene_id}', f'ground_truth_{timestamp}_{outTif}')\n",
    "                    with rasterio.open(truth_filename, \"w\", **profile) as dst:\n",
    "                        dst.write(targets_np.astype(np.float32), 1)\n",
    "                    \n",
    "                    # Copy original out file\n",
    "                    orig_filename = os.path.join(output_dir, f'original_{scene_id}_{outTif}')                    \n",
    "                    if not os.path.exists(orig_filename):\n",
    "                        # print(f\"Original LST: {os.path.basename(orig_filename)}\")\n",
    "                        shutil.copy2(ground_truth_file, orig_filename)\n",
    "                    \n",
    "                    # Calculate metrics for valid pixels\n",
    "                    valid_mask = predicted_np != -9999\n",
    "                    if valid_mask.any():\n",
    "                        mae = np.mean(np.abs(predicted_np[valid_mask] - targets_np[valid_mask]))\n",
    "                        rmse = np.sqrt(np.mean((predicted_np[valid_mask] - targets_np[valid_mask])**2))\n",
    "                        metrics = {'mae': mae, 'rmse': rmse}\n",
    "                #         print(f\"Predictions: {os.path.basename(pred_filename)}\")\n",
    "                #         # print(f\"Ground Truth: {os.path.basename(truth_filename)}\")\n",
    "                #         if 'Heat' in pred_filename:\n",
    "                #             print(f\"Mean Absolute Error: {mae:.2f} points.\")\n",
    "                #             print(f\"Root Mean Square Error: {rmse:.2f} points.\")\n",
    "                #         else:\n",
    "                #             print(f\"Mean Absolute Error: {mae:.2f}°F\")\n",
    "                #             print(f\"Root Mean Square Error: {rmse:.2f}°F\")\n",
    "                # print(f\"\\nSaved files in {output_dir}/:\")\n",
    "    # print(f'There are {len(scenes)} scenes')\n",
    "    for scene in scenes:\n",
    "        combine_scene_tifs(scene)\n",
    "def inference(model, test_loader, tiles_count: int, device='cuda', denormalize: bool=False):\n",
    "    model = model.to(device)\n",
    "    model.eval()    \n",
    "    batch = 0\n",
    "    it = iter(test_loader)\n",
    "    scenes = set()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(tiles_count):\n",
    "            sleep(1)\n",
    "            # Get one sample\n",
    "            sample = next(it)\n",
    "            for l, outTif in enumerate(['LST.tif', 'HeatIndex.tif']):\n",
    "                inputs = sample['input'].to(device)\n",
    "                targets = sample['target'].to(device)\n",
    "                mask = sample['mask'].to(device)\n",
    "                ground_truth_file = sample['file_dict'][outTif][0]\n",
    "                box = sample['box']\n",
    "                box = [int(tensor.item()) for tensor in box]\n",
    "\n",
    "                # Get model prediction\n",
    "                outputs = model(inputs)\n",
    "                if denormalize:\n",
    "                    print(outputs.shape)\n",
    "                    outputs = TiledGeotiffDataset.denormalize(outputs)[batch][l]\n",
    "                    targets = TiledGeotiffDataset.denormalize(targets)[batch][l]\n",
    "\n",
    "                # Move to CPU and convert to numpy\n",
    "                mask_np = mask.cpu().numpy().squeeze()\n",
    "                targets_np = targets.cpu().numpy().squeeze()\n",
    "                predicted_np = outputs.cpu().numpy().squeeze()\n",
    "                \n",
    "                # Apply mask\n",
    "                predicted_np[~mask_np] = -9999\n",
    "                targets_np[~mask_np] = -9999\n",
    "\n",
    "                output_dir = \"./Data/prediction\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                window = Window(col_off=xmin, row_off=ymin, width=xmax-xmin, height=ymax-ymin)\n",
    "                \n",
    "                # Get corresponding LST file path and save outputs\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                with rasterio.open(ground_truth_file) as src:\n",
    "                    profile = src.profile.copy()\n",
    "                    # Update the transform based on the window\n",
    "                    window_transform = rasterio.windows.transform(window, src.transform)\n",
    "                    \n",
    "                    # Update profile with new dimensions and transform\n",
    "                    profile.update(\n",
    "                        width=xmax-xmin,\n",
    "                        height=ymax-ymin,\n",
    "                        transform=window_transform,\n",
    "                        count=1,\n",
    "                        nodata=-9999\n",
    "                    )                                                    \n",
    "                    \n",
    "                    # Save prediction\n",
    "                    scene_id = f\"{ground_truth_file.split('/')[-3]}_{ground_truth_file.split('/')[-2]}\"  # city_date\n",
    "                    scenes.add(os.path.join(output_dir, scene_id))\n",
    "                    os.makedirs(os.path.join(output_dir, scene_id), exist_ok=True)\n",
    "                    pred_filename = os.path.join(output_dir, scene_id, f'predicted_{timestamp}_{outTif}')\n",
    "                    with rasterio.open(pred_filename, \"w\", **profile) as dst:\n",
    "                        dst.write(predicted_np.astype(np.float32), 1)\n",
    "                    \n",
    "                    # Save ground truth\n",
    "                    os.makedirs(os.path.join(output_dir, f'ground_truth_{scene_id}'), exist_ok=True)\n",
    "                    truth_filename = os.path.join(output_dir, f'ground_truth_{scene_id}', f'ground_truth_{timestamp}_{outTif}')\n",
    "                    with rasterio.open(truth_filename, \"w\", **profile) as dst:\n",
    "                        dst.write(targets_np.astype(np.float32), 1)\n",
    "                    \n",
    "                    # Copy original out file\n",
    "                    orig_filename = os.path.join(output_dir, f'original_{scene_id}_{outTif}')                    \n",
    "                    if not os.path.exists(orig_filename):\n",
    "                        print(f\"Original LST: {os.path.basename(orig_filename)}\")\n",
    "                        shutil.copy2(ground_truth_file, orig_filename)\n",
    "                    \n",
    "                    # Calculate metrics for valid pixels\n",
    "                    valid_mask = predicted_np != -9999\n",
    "                    if valid_mask.any():\n",
    "                        mae = np.mean(np.abs(predicted_np[valid_mask] - targets_np[valid_mask]))\n",
    "                        rmse = np.sqrt(np.mean((predicted_np[valid_mask] - targets_np[valid_mask])**2))\n",
    "                        metrics = {'mae': mae, 'rmse': rmse}\n",
    "                        print(f\"Predictions: {os.path.basename(pred_filename)}\")\n",
    "                        # print(f\"Ground Truth: {os.path.basename(truth_filename)}\")\n",
    "                        if 'Heat' in pred_filename:\n",
    "                            print(f\"Mean Absolute Error: {mae:.2f} points.\")\n",
    "                            print(f\"Root Mean Square Error: {rmse:.2f} points.\")\n",
    "                        else:\n",
    "                            print(f\"Mean Absolute Error: {mae:.2f}°F\")\n",
    "                            print(f\"Root Mean Square Error: {rmse:.2f}°F\")\n",
    "                print(f\"\\nSaved files in {output_dir}/:\")\n",
    "    print(f'There are {len(scenes)} scenes')\n",
    "    for scene in scenes:\n",
    "        combine_scene_tifs(scene)\n",
    "    \n",
    "    # return metrics\n",
    "\n",
    "def test_data_quality(test_loader, tiles_count: int, denormalize: bool = False):\n",
    "    batch = 0\n",
    "    it = iter(test_loader)\n",
    "    for _ in range(tiles_count):\n",
    "        sleep(1)\n",
    "        # Get one sample\n",
    "        sample = next(it)\n",
    "        printedOriginals = set()\n",
    "        if denormalize:\n",
    "            sample = TiledGeotiffDataset.denormalize(sample)\n",
    "        for i, tif in enumerate(['Albedo.tif', 'DEM.tif', 'Land_Cover.tif', 'NDVI.tif', 'NDWI.tif', 'NDBI.tif', 'LST.tif', 'HeatIndex.tif']):\n",
    "            with torch.no_grad():\n",
    "                if i <= 5:\n",
    "                    targets = sample['input'][batch][i]\n",
    "                else:\n",
    "                    targets = sample['target'][batch][i-6]\n",
    "                mask = sample['mask']\n",
    "                target_file_origin = sample['file_dict'][tif][0]\n",
    "                scene_id = str(target_file_origin.split('/')[-3]) + '_' + str(target_file_origin.split('/')[-2]) + '_' + str(tif)\n",
    "                box = sample['box']\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                box = [int(tensor.item()) for tensor in box]\n",
    "                # print(box)\n",
    "\n",
    "                # Move to CPU and convert to numpy\n",
    "                mask_np = mask.cpu().numpy().squeeze()\n",
    "                targets_np = targets.cpu().numpy().squeeze()\n",
    "\n",
    "                noData = -9999\n",
    "                targets_np[~mask_np] = noData\n",
    "                # Save ground truth\n",
    "                output_dir = \"./Data/truth\"\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                window = Window(col_off=xmin, row_off=ymin, width=xmax-xmin, height=ymax-ymin)\n",
    "                \n",
    "                print(target_file_origin)\n",
    "                with rasterio.open(target_file_origin) as src:\n",
    "                    # Get the profile from the source\n",
    "                    test_profile = src.profile.copy()\n",
    "                    \n",
    "                    # Update the transform based on the window\n",
    "                    window_transform = rasterio.windows.transform(window, src.transform)\n",
    "                    \n",
    "                    # Update profile with new dimensions and transform\n",
    "                    test_profile.update(\n",
    "                        width=xmax-xmin,\n",
    "                        height=ymax-ymin,\n",
    "                        transform=window_transform,\n",
    "                        count=1,\n",
    "                        nodata=noData\n",
    "                    )\n",
    "                    \n",
    "                    truth_filename = os.path.join(output_dir, f'ground_truth_{timestamp}_{tif}')\n",
    "                    with rasterio.open(truth_filename, \"w\", **test_profile) as dst:\n",
    "                        dst.write(targets_np.astype(np.float32), 1)\n",
    "\n",
    "                orig_filename = os.path.join(output_dir, f'original_File_{scene_id}')\n",
    "                if os.path.exists(orig_filename):\n",
    "                    print(f\"Original file already exists: {orig_filename}\")\n",
    "                    continue\n",
    "                with rasterio.open(target_file_origin) as src:\n",
    "                    profile = src.profile.copy()\n",
    "                    profile.update(dtype='float32', count=1, nodata=np.nan)\n",
    "                    print(f\"\\nSaved files in {output_dir}/:\")\n",
    "                    print(f\"Ground Truth: {os.path.basename(truth_filename)}\")\n",
    "                    print(f\"Original LST: {os.path.basename(orig_filename)}\")\n",
    "                    # Copy original output files\n",
    "                    print(f'Copying {target_file_origin} to {orig_filename}')\n",
    "                    if not os.path.exists(orig_filename):\n",
    "                        shutil.copy2(target_file_origin, orig_filename)\n",
    "\n",
    "def test_sizes(test_loader, num_samples=None):\n",
    "    \"\"\"\n",
    "    Test if all X, y from the dataloader have the same image sizes.\n",
    "    \n",
    "    Args:\n",
    "        test_loader: DataLoader to test\n",
    "        num_samples: Number of samples to check (default: all)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if all consistent, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"Testing image sizes in dataloader...\")\n",
    "    \n",
    "    # Get the first batch to establish expected shapes\n",
    "    it = iter(test_loader)\n",
    "    first_sample = next(it)\n",
    "    \n",
    "    # Extract shapes\n",
    "    input_shape = first_sample['input'].shape\n",
    "    target_shape = first_sample['target'].shape\n",
    "    mask_shape = first_sample['mask'].shape\n",
    "    \n",
    "    print(f\"Expected input shape: {input_shape}\")\n",
    "    print(f\"Expected target shape: {target_shape}\")\n",
    "    print(f\"Expected mask shape: {mask_shape}\")\n",
    "    \n",
    "    # Check if the shapes are as expected (6 input channels, 2 target channels)\n",
    "    if input_shape[1] != 6:\n",
    "        print(f\"WARNING: Expected 6 input channels, got {input_shape[1]}\")\n",
    "    if target_shape[1] != 2:\n",
    "        print(f\"WARNING: Expected 2 target channels, got {target_shape[1]}\")\n",
    "    \n",
    "    # Reset iterator\n",
    "    it = iter(test_loader)\n",
    "    all_consistent = True\n",
    "    count = 0\n",
    "    \n",
    "    # Define the expected input files\n",
    "    expected_files = ['Albedo.tif', 'DEM.tif', 'Land_Cover.tif', 'NDVI.tif', 'NDWI.tif', 'NDBI.tif', 'LST.tif', 'HeatIndex.tif']\n",
    "    \n",
    "    # Iterate through samples\n",
    "    total_samples = len(test_loader) if num_samples is None else min(num_samples, len(test_loader))\n",
    "    \n",
    "    for i in range(total_samples):\n",
    "        try:\n",
    "            sample = next(it)\n",
    "            \n",
    "            # Check shapes\n",
    "            current_input_shape = sample['input'].shape\n",
    "            current_target_shape = sample['target'].shape\n",
    "            current_mask_shape = sample['mask'].shape\n",
    "            \n",
    "            # Verify file dictionary contains all expected files\n",
    "            file_dict = sample['file_dict']\n",
    "            missing_files = [f for f in expected_files if f not in file_dict]\n",
    "            \n",
    "            if current_input_shape != input_shape:\n",
    "                print(f\"Sample {i}: Input shape mismatch! Expected {input_shape}, got {current_input_shape}\")\n",
    "                all_consistent = False\n",
    "            \n",
    "            if current_target_shape != target_shape:\n",
    "                print(f\"Sample {i}: Target shape mismatch! Expected {target_shape}, got {current_target_shape}\")\n",
    "                all_consistent = False\n",
    "            \n",
    "            if current_mask_shape != mask_shape:\n",
    "                print(f\"Sample {i}: Mask shape mismatch! Expected {mask_shape}, got {current_mask_shape}\")\n",
    "                all_consistent = False\n",
    "            \n",
    "            if missing_files:\n",
    "                print(f\"Sample {i}: Missing files: {missing_files}\")\n",
    "                all_consistent = False\n",
    "            \n",
    "            # Check each file in the file_dict has consistent number of entries\n",
    "            for file_type, file_list in file_dict.items():\n",
    "                if len(file_list) != 1:  # Assuming each should have exactly 1 file\n",
    "                    print(f\"Sample {i}: File type {file_type} has {len(file_list)} entries instead of 1\")\n",
    "                    all_consistent = False\n",
    "            \n",
    "            # Print progress\n",
    "            count += 1\n",
    "            if count % 10 == 0:\n",
    "                print(f\"Checked {count}/{total_samples} samples...\")\n",
    "                \n",
    "        except StopIteration:\n",
    "            break\n",
    "    \n",
    "    # Final summary\n",
    "    if all_consistent:\n",
    "        print(f\"✅ All {count} samples have consistent shapes!\")\n",
    "        print(f\"Input shape: {input_shape}\")\n",
    "        print(f\"Target shape: {target_shape}\")\n",
    "        print(f\"Mask shape: {mask_shape}\")\n",
    "    else:\n",
    "        print(f\"❌ Found inconsistencies in the {count} samples checked.\")\n",
    "    \n",
    "    return all_consistent\n",
    "\n",
    "import os\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "def get_file_paths(folder_path: str) -> list[str]:\n",
    "        file_paths = []\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                full_path = os.path.abspath(os.path.join(root, file))\n",
    "                file_paths.append(full_path)\n",
    "        return file_paths   \n",
    "def test_original_file_dimensions(data_dir, months_ahead=3, include_years=None):\n",
    "    \"\"\"\n",
    "    Test if all original TIF files in X and y folders have consistent dimensions.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Base data directory\n",
    "        months_ahead: Months ahead for prediction (0, 3, 6)\n",
    "        include_years: List of years to include (e.g., [\"2018\", \"2019\"])\n",
    "    \"\"\"\n",
    "    if include_years is None:\n",
    "        include_years = [\"2018\", \"2019\", \"2020\"]\n",
    "    \n",
    "    print(f\"Testing original TIF files in preprocess_{months_ahead}monthsahead...\")\n",
    "    \n",
    "    # Expected input and output files\n",
    "    input_files = ['Albedo.tif', 'DEM.tif', 'Land_Cover.tif', 'NDVI.tif', 'NDWI.tif', 'NDBI.tif']\n",
    "    output_files = ['LST.tif', 'HeatIndex.tif']\n",
    "    \n",
    "    # Store file dimensions for each scene\n",
    "    scene_dimensions = {}\n",
    "    inconsistent_scenes = []\n",
    "    \n",
    "    # Find all Albedo files as hooks\n",
    "    albedo_files = []\n",
    "    x_dir = os.path.join(data_dir, f'preprocess_{months_ahead}monthsahead', 'X', 'less5CloudCover')\n",
    "    for file_path in tqdm(get_file_paths(x_dir), desc='Gathering scenes...'):\n",
    "        date = file_path.split('/')[-2]\n",
    "        for year in include_years:\n",
    "            if year in date:\n",
    "                if 'Albedo' in file_path:\n",
    "                    albedo_files.append(file_path)\n",
    "    \n",
    "    print(f\"Found {len(albedo_files)} Albedo files to check\")\n",
    "    for albedo_path in tqdm(albedo_files, desc='Checking file dimensions'):\n",
    "        # Get scene info from albedo path\n",
    "        scene_dir = os.path.dirname(albedo_path)\n",
    "        scene_id = f\"{scene_dir.split('/')[-3]}_{scene_dir.split('/')[-2]}\"  # city_date\n",
    "        \n",
    "        # Initialize dimension data for this scene\n",
    "        scene_dimensions[scene_id] = {'input': {}, 'output': {}}\n",
    "        has_consistency = True\n",
    "        \n",
    "        # Check input files\n",
    "        for input_file in input_files:\n",
    "            input_path = os.path.join(scene_dir, input_file)\n",
    "            if os.path.exists(input_path):\n",
    "                try:\n",
    "                    with rasterio.open(input_path) as src:\n",
    "                        width, height = src.width, src.height\n",
    "                        scene_dimensions[scene_id]['input'][input_file] = (width, height)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {input_path}: {e}\")\n",
    "                    has_consistency = False\n",
    "            else:\n",
    "                print(f\"Missing input file: {input_path}\")\n",
    "                has_consistency = False\n",
    "        \n",
    "        # Check corresponding output files\n",
    "        date = scene_dir.split('/')[-1]\n",
    "        date_object = datetime.strptime(date, \"%Y-%m\")\n",
    "        date_object = date_object + relativedelta(months=months_ahead)\n",
    "        dateAhead = date_object.strftime(\"%Y-%m\")\n",
    "        y_dir = albedo_path.replace('/X/', '/y/').replace(date, dateAhead).replace('Albedo.tif', '')\n",
    "        for output_file in output_files:\n",
    "            output_path = os.path.join(y_dir, output_file)\n",
    "            if os.path.exists(output_path):\n",
    "                try:\n",
    "                    with rasterio.open(output_path) as src:\n",
    "                        width, height = src.width, src.height\n",
    "                        scene_dimensions[scene_id]['output'][output_file] = (width, height)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {output_path}: {e}\")\n",
    "                    has_consistency = False\n",
    "            else:\n",
    "                print(f\"Missing output file: {output_path}\")\n",
    "                has_consistency = False\n",
    "\n",
    "    for id in tqdm(scene_dimensions.keys(), desc='Checking for inconsistencies'):\n",
    "        input = set(scene_dimensions[id]['input'].values())\n",
    "        output = set(scene_dimensions[id]['output'].values())\n",
    "        if len(input | output) > 1:\n",
    "            inconsistent_scenes.append(id)\n",
    "            print(f\"Error: {id} has different sizes\")\n",
    "            has_consistency = False\n",
    "    print(\"The consistency is \" + str(has_consistency))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640ee65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/ubh496/.conda/envs/ml/lib/python3.10/site-packages/torch/hub.py:846: UserWarning: TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead\n",
      "  warnings.warn(\n",
      "Gathering scenes (Sort by Random Scene)...: 100%|██████████| 2226/2226 [00:00<00:00, 1272352.24it/s]\n",
      "Preparing scene by scene...: 100%|██████████| 318/318 [00:00<00:00, 371.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits - Train: 254, Val: 32, Test: 32\n",
      "Test is length 1490\n",
      "Validate is length 1466\n",
      "Train is length 12776\n"
     ]
    }
   ],
   "source": [
    "from utils.data.TiledLandsatDataModule import TiledLandsatDataModule\n",
    "from utils.model import LSTNowcaster\n",
    "\n",
    "config = {\n",
    "    \"debug\": True,\n",
    "    \"augment\": False,\n",
    "    \"by_city\": False,\n",
    "    \"tile_size\": 128,\n",
    "    \"tile_overlap\": 0.0,\n",
    "    \"months_ahead\": 1,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model\": \"segformer\",\n",
    "    \"backbone\": \"b5\",\n",
    "    \"dataset\": \"pure_landsat\",\n",
    "    \"epochs\": 25,\n",
    "    \"batch_size\": 1,\n",
    "    \"pretrained_weights\": True,\n",
    "    \"deterministic\": True,\n",
    "    \"in_channels\": 7\n",
    "}\n",
    "\n",
    "best_model = LSTNowcaster()\n",
    "\n",
    "best_model = LSTNowcaster.load_from_checkpoint(\n",
    "    \"/work/ubh496/testDataset/lst-benchmark/wandb/heat-island/checkpoints/f9smq9br_May23/f9smq9br_May23_epoch=001_val_rmse_F=11.0967.ckpt\"\n",
    ")\n",
    "\n",
    "data_module = TiledLandsatDataModule(\n",
    "    debug=config['debug'],\n",
    "    data_dir=\"./Data\",\n",
    "    monthsAhead=config[\"months_ahead\"],\n",
    "    augment=config[\"augment\"],\n",
    "    shuffleTrain=False,\n",
    "    batch_size=1,\n",
    "    num_workers=5,        \n",
    "    tile_size=config[\"tile_size\"],\n",
    "    includeYears=[\"2022\",\"2023\"]\n",
    ")\n",
    "# Setup the data module to prepare datasets\n",
    "data_module.setup()\n",
    "print(f'Test is length {len(data_module.test_dataloader())}')\n",
    "print(f'Validate is length {len(data_module.val_dataloader())}')\n",
    "print(f'Train is length {len(data_module.train_dataloader())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262f9231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Atlanta tiles...:  74%|███████▍  | 9516/12776 [00:45<00:15, 211.87it/s]/work/ubh496/testDataset/lst-benchmark/utils/model.py:104: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "Processing Atlanta tiles...: 100%|██████████| 12776/12776 [01:48<00:00, 117.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42 files for LST.tif\n",
      "['./Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134429_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134422_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134457_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134505_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134502_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134445_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134503_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134426_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134428_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134456_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134421_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134459_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134441_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134436_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134451_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134454_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134432_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134443_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134423_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134435_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134438_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134444_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134425_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134437_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134433_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134447_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134446_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134458_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134427_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134424_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134450_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134448_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134440_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134419_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134439_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134455_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134430_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134452_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134501_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134500_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134434_LST.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134449_LST.tif']\n",
      "Combined LST.tif saved as: ./Data/prediction/Atlanta_GA_2014-02_combined_LST.tif\n",
      "Found 42 files for HeatIndex.tif\n",
      "['./Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134459_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134445_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134433_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134505_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134502_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134430_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134447_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134454_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134438_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134500_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134424_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134427_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134501_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134440_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134439_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134436_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134435_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134419_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134434_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134456_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134457_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134423_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134504_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134441_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134446_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134437_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134422_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134453_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134443_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134429_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134448_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134451_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134425_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134444_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134432_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134450_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134428_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134449_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134426_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134421_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134458_HeatIndex.tif', './Data/prediction/Atlanta_GA_2014-02/predicted_20250523_134455_HeatIndex.tif']\n",
      "Combined HeatIndex.tif saved as: ./Data/prediction/Atlanta_GA_2014-02_combined_HeatIndex.tif\n"
     ]
    }
   ],
   "source": [
    "inferenceCity(\n",
    "    city='Atlanta',model=best_model, test_loader=data_module.train_dataloader(), denormalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8477c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inferenceRaster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Use file Paths\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minferenceRaster\u001b[49m(\n\u001b[1;32m      3\u001b[0m     ndvi\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     ndwi\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     ndbi\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     albedo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/Albedo.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     dem\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/DEM.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     land_cover\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/Land_Cover.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inferenceRaster' is not defined"
     ]
    }
   ],
   "source": [
    "#Use file Paths\n",
    "inferenceRaster(\n",
    "    ndvi=\"/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/NDVI.tif\",\n",
    "    ndwi=\"/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/NDWI.tif\",\n",
    "    ndbi=\"/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/NDBI.tif\",\n",
    "    albedo=\"/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/Albedo.tif\",\n",
    "    dem=\"/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/DEM.tif\",\n",
    "    land_cover=\"/work/ubh496/testDataset/lst-benchmark/Data/preprocess_1monthsahead/X/less5CloudCover/Abilene_TX/2014-05/Land_Cover.tif\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchgeo.trainers import PixelwiseRegressionTask\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import cv2\n",
    "import logging\n",
    "from typing import List\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "from utils.model import LSTNowcaster\n",
    "from utils.data.TiledLandsatDataModule import TiledLandsatDataModule\n",
    "from utils.voice import notifySelf\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"TrainUNet-Basic.ipynb\"\n",
    "os.environ[\"WANDB_DIR\"] = \"./wandb\"\n",
    "os.environ[\"WANDB_CACHE_DIR\"] = \"./wandb/.cache/wandb\"\n",
    "os.environ[\"WANDB_CONFIG_DIR\"] = \"./wandb/.config/wandb\"\n",
    "os.environ[\"WANDB_DATA_DIR\"] = \"./wandb/.cache/wandb-data\"\n",
    "os.environ[\"WANDB_ARTIFACT_DIR\"] = \"./wandb/artifacts\"\n",
    "import sys\n",
    "\n",
    "i = -1\n",
    "# Get the first argument passed after the script name\n",
    "if len(sys.argv) > 1:\n",
    "    i = int(sys.argv[1])  # Convert string to integer\n",
    "    batchSize = int(sys.argv[2])\n",
    "    deviceCount = int(sys.argv[3])\n",
    "config = {\n",
    "    \"experiment_name\": \"7 channels 3 months debug segformer\",\n",
    "    \"debug\": False,\n",
    "    \"by_city\": False,\n",
    "    \"months_ahead\": 1,\n",
    "    \"tile_size\": 128,\n",
    "    \"tile_overlap\": 0.0,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"model\": \"segformer\",\n",
    "    \"backbone\": \"b5\",\n",
    "    \"dataset\": \"pure_landsat\",\n",
    "    \"augment\": True,\n",
    "    \"epochs\": 160,\n",
    "    \"batch_size\": batchSize,\n",
    "    \"pretrained_weights\": True,\n",
    "    \"deterministic\": True,\n",
    "    \"random_seed_by_scene\": 1,\n",
    "    \"in_channels\": 7,\n",
    "    \"only_train\": False,\n",
    "    \"skip_years\": []\n",
    "}\n",
    "\n",
    "if i == 1:        \n",
    "    config[\"model\"] = \"segformer\"\n",
    "    config[\"backbone\"] = \"b5\"\n",
    "    config[\"months_ahead\"] = 1\n",
    "if i == 2:        \n",
    "    config[\"model\"] = \"segformer\"\n",
    "    config[\"backbone\"] = \"b5\"\n",
    "    config[\"months_ahead\"] = 3\n",
    "if i == 3:        \n",
    "    config[\"model\"] = \"segformer\"\n",
    "    config[\"backbone\"] = \"b3\"\n",
    "    config[\"months_ahead\"] = 1\n",
    "if i == 4:        \n",
    "    config[\"model\"] = \"segformer\"\n",
    "    config[\"backbone\"] = \"b3\"\n",
    "    config[\"months_ahead\"] = 3\n",
    "if i == 5:        \n",
    "    config[\"model\"] = \"deeplabv3+\"\n",
    "    config[\"backbone\"] = \"resnet50\"\n",
    "    config[\"months_ahead\"] = 1\n",
    "if i == 6:        \n",
    "    config[\"model\"] = \"deeplabv3+\"\n",
    "    config[\"backbone\"] = \"resnet50\"\n",
    "    config[\"months_ahead\"] = 3\n",
    "#b3\n",
    "if i == 7:        \n",
    "    config[\"model\"] = \"deeplabv3+\"\n",
    "    config[\"backbone\"] = \"resnet18\"\n",
    "    config[\"months_ahead\"] = 1\n",
    "if i == 8:        \n",
    "    config[\"model\"] = \"deeplabv3+\"\n",
    "    config[\"backbone\"] = \"resnet18\"\n",
    "    config[\"months_ahead\"] = 3\n",
    "if i == 9:        \n",
    "    config[\"model\"] = \"unet\"\n",
    "    config[\"backbone\"] = \"resnet50\"\n",
    "    config[\"months_ahead\"] = 1\n",
    "if i == 10:        \n",
    "    config[\"model\"] = \"unet\"\n",
    "    config[\"backbone\"] = \"resnet50\"\n",
    "    config[\"months_ahead\"] = 3\n",
    "if i == 11:        \n",
    "    config[\"model\"] = \"unet\"\n",
    "    config[\"backbone\"] = \"resnet18\"\n",
    "    config[\"months_ahead\"] = 1\n",
    "if i == 12:        \n",
    "    config[\"model\"] = \"unet\"\n",
    "    config[\"backbone\"] = \"resnet18\"\n",
    "    config[\"months_ahead\"] = 3\n",
    "if i <= -1:\n",
    "    pass\n",
    "else:\n",
    "    config[\"experiment_name\"] = f'Exp. #{i}: {config[\"model\"]},Month {config[\"months_ahead\"]}, {config[\"backbone\"]}'\n",
    "\n",
    "notifySelf(f'Starting {config[\"experiment_name\"]}!')\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"heat-island\",\n",
    "    name=config['experiment_name'],\n",
    "    log_model=\"best\",\n",
    "    save_code=True,\n",
    "    save_dir=\"./wandb\",\n",
    ")\n",
    "wandb_logger.log_hyperparams(config)    \n",
    "\n",
    "# Create model\n",
    "model = LSTNowcaster(\n",
    "    model=config[\"model\"], \n",
    "    backbone=config[\"backbone\"], \n",
    "    in_channels=config[\"in_channels\"], \n",
    "    learning_rate=config[\"learning_rate\"], \n",
    "    pretrained_weights=config[\"pretrained_weights\"]\n",
    ")\n",
    "\n",
    "class PercentageProgressCallback(Callback):\n",
    "    def __init__(self, total_epochs, experiment_name):\n",
    "        super().__init__()\n",
    "        self.total_epochs = total_epochs\n",
    "        self.experiment_name = experiment_name\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Only run on main process\n",
    "        if trainer.is_global_zero:\n",
    "            current_epoch = trainer.current_epoch\n",
    "            if current_epoch % 20 == 0:\n",
    "                current_percentage = min(100, int(current_epoch / self.total_epochs * 100))\n",
    "                wandb.alert(title=\"Training Update\", \n",
    "                        text=f'{self.experiment_name} is at {current_percentage:.2f}%', \n",
    "                        level=wandb.AlertLevel.INFO)\n",
    "\n",
    "percentage_callback = PercentageProgressCallback(total_epochs=config[\"epochs\"], experiment_name=config[\"experiment_name\"])    \n",
    "wandb_run_id = wandb_logger.experiment.id    \n",
    "current_date = datetime.now()                \n",
    "date_string = current_date.strftime(\"%B%d\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"./wandb/heat-island/checkpoints/{wandb_run_id}_{date_string}\",\n",
    "    filename= f\"{wandb_run_id}_{date_string}_\" + \"{epoch:03d}_{val_rmse_F:.4f}\",\n",
    "    monitor=\"val_rmse_p\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    every_n_epochs=1,\n",
    "    save_last=False  # Also save the last model for comparison\n",
    ")\n",
    "allYears = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "for year in config[\"skip_years\"]:\n",
    "    allYears.remove(year)\n",
    "# for subYears in [allYears[:5], allYears[5:]]:\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config['epochs'],\n",
    "    gradient_clip_val=0.5,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=False,\n",
    "    # deterministic=config[\"deterministic\"],\n",
    "    num_sanity_val_steps=2,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, percentage_callback],\n",
    "    # devices=deviceCount,                         # Use all 4 GPUs\n",
    "    accelerator=\"gpu\",                 # Use GPU acceleration\n",
    "    # strategy=\"ddp\",                    # Use DistributedDataParallel\n",
    "    precision=\"16-mixed\"               # Add mixed precision for memory efficiency\n",
    ")                             \n",
    "\n",
    "data_module = TiledLandsatDataModule(\n",
    "    data_dir=\"./Data\",\n",
    "    monthsAhead=config[\"months_ahead\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=8,\n",
    "    byCity=config[\"by_city\"],\n",
    "    debug=config[\"debug\"],\n",
    "    tile_size=config[\"tile_size\"],\n",
    "    tile_overlap=config[\"tile_overlap\"],\n",
    "    augment=config[\"augment\"],\n",
    "    seedForScene=config[\"random_seed_by_scene\"],\n",
    "    onlyTrain = config[\"only_train\"],\n",
    "    includeYears=allYears\n",
    ")\n",
    "data_module.setup()\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model=model, datamodule=data_module)\n",
    "del trainer\n",
    "del data_module\n",
    "# Force garbage collection and clear CUDA cache\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# After deleting objects\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    with torch.cuda.device(i):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Register the best model as a W&B artifact\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "if best_model_path and os.path.exists(best_model_path):\n",
    "    artifact = wandb.Artifact(\n",
    "        name=f\"{best_model_path.split('/')[-1].replace('=','.')}\", \n",
    "        type=\"model\",\n",
    "        description=f\"Best model at {best_model_path.split('/')[-1]}\" \n",
    "    )\n",
    "    artifact.add_file(best_model_path)\n",
    "    wandb_logger.experiment.log_artifact(artifact)\n",
    "\n",
    "# End Experiment\n",
    "wandb.finish()\n",
    "notifySelf(f\"Finished {config['experiment_name']}...\")\n",
    "del model\n",
    "del wandb_logger\n",
    "del checkpoint_callback\n",
    "\n",
    "# Force garbage collection and clear CUDA cache\n",
    "import gc\n",
    "for obj in gc.get_objects():   \n",
    "    try:\n",
    "        if torch.is_tensor(obj) and obj.device.type == 'cuda':\n",
    "            del obj\n",
    "    except:\n",
    "        pass\n",
    "gc.collect()\n",
    "\n",
    "# After deleting objects\n",
    "for j in range(torch.cuda.device_count()):\n",
    "    with torch.cuda.device(j):\n",
    "        x = torch.zeros(1024, 1024, 1024, device=f'cuda:{j}')\n",
    "        del x\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "# 4. Wait for GPU processes to complete\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Print memory stats for debugging\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Loop {i} completed. CUDA memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"CUDA memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "notifySelf(\"Batch experiment ended.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, load the best checkpoint for testing\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model path: {best_model_path}\")\n",
    "\n",
    "if best_model_path:\n",
    "    # Load best checkpoint\n",
    "    best_model = LSTNowcaster.load_from_checkpoint(\n",
    "        checkpoint_path=best_model_path,\n",
    "        model=config[\"model\"],\n",
    "        backbone=config[\"backbone\"],\n",
    "        in_channels=config[\"in_channels\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        pretrained_weights=config[\"pretrained_weights\"]\n",
    "    )\n",
    "    \n",
    "    # Test using the best model\n",
    "    trainer.test(model=best_model, datamodule=data_module)\n",
    "else:\n",
    "    print(\"No checkpoint found, testing with the current model state\")\n",
    "    trainer.test(model=model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6d9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from utils.model import LSTNowcaster\n",
    "from utils.data.TiledLandsatDataModule import TiledLandsatDataModule\n",
    "\n",
    "# Define which model checkpoint to test\n",
    "# You can either specify a specific checkpoint or use the best one from a previous run\n",
    "for checkpoint_path in [\n",
    "    \"/home/ubuntu/heat-island-test/wandb/heat-island/checkpoints/up47iayb_April15/up47iayb_April15_epoch=059_val_rmse_F=17.0594.ckpt\"\n",
    "]:\n",
    "\n",
    "    # Initialize test configuration\n",
    "    test_config = {\n",
    "        \"experiment_name\": \"Test OneFormer Debug\",\n",
    "        \"debug\": True,  # Set to False for full test\n",
    "        \"by_city\": False,\n",
    "        \"months_ahead\": 3,\n",
    "        \"tile_size\": 128,\n",
    "        \"tile_overlap\": 0.0,\n",
    "        \"model\": \"segformer\",\n",
    "        \"backbone\": \"b5\",\n",
    "        \"dataset\": \"pure_landsat\",\n",
    "        \"batch_size\": 1,  # Can be larger than training since no gradients are stored\n",
    "        \"in_channels\": 6\n",
    "    }\n",
    "\n",
    "    # Get the run ID from your checkpoint path\n",
    "    run_id = checkpoint_path.split('/')[-2].split('_')[0]  # Extracts the run ID from the checkpoint path\n",
    "\n",
    "    # Initialize WandB logger that continues the same run\n",
    "    test_logger = WandbLogger(\n",
    "        project=\"heat-island\",\n",
    "        id=run_id,  # Use the same run ID to continue logging to the same run\n",
    "        resume=\"must\",  # Force resume the existing run\n",
    "        save_dir=\"./wandb\",\n",
    "    )\n",
    "\n",
    "    # Set up data module for testing\n",
    "    data_module = TiledLandsatDataModule(\n",
    "        data_dir=\"./Data\",\n",
    "        monthsAhead=test_config[\"months_ahead\"],\n",
    "        batch_size=test_config[\"batch_size\"],\n",
    "        num_workers=4,\n",
    "        byCity=test_config[\"by_city\"],\n",
    "        debug=test_config[\"debug\"],\n",
    "        tile_size=test_config[\"tile_size\"],\n",
    "        tile_overlap=test_config[\"tile_overlap\"],\n",
    "        augment=False,  # No augmentation during testing\n",
    "        seedForScene=1,  # Consistent seed for reproducibility\n",
    "        includeYears=[\"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "    )\n",
    "    data_module.setup()  # Explicitly prepare the test data\n",
    "\n",
    "    # Initialize the model with the same architecture used during training\n",
    "    model = LSTNowcaster.load_from_checkpoint(\n",
    "        checkpoint_path,\n",
    "        model=test_config[\"model\"],\n",
    "        backbone=test_config[\"backbone\"],\n",
    "        in_channels=test_config[\"in_channels\"]\n",
    "    )\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize trainer specifically for testing\n",
    "    from pytorch_lightning import Trainer\n",
    "    test_trainer = Trainer(\n",
    "        logger=test_logger,\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    # Run test\n",
    "    test_results = test_trainer.test(model=model, datamodule=data_module)\n",
    "\n",
    "    # Log detailed test metrics\n",
    "    test_logger.experiment.log({\n",
    "        \"test_results\": test_results[0],\n",
    "        \"test_rmse_F\": test_results[0].get(\"test_rmse_F\", None),\n",
    "        \"test_mae_F\": test_results[0].get(\"test_mae_F\", None)\n",
    "    })\n",
    "\n",
    "    # Clean up resources\n",
    "    del model\n",
    "    del test_trainer\n",
    "    del data_module\n",
    "    del test_logger\n",
    "\n",
    "    # Force garbage collection and clear CUDA cache\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Test complete. Results: {test_results}\")\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
